{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAHF9JREFUeJzt3UmPZOl1HuAvxsyMrKzKqsqau6rYA5vNbropkjJJmYIs\nUIBXWtn+BV7YO/8Yr73wymtDNAwIggwSMEmBNMeW2Wz2VOzumquyco6M2QttzI2Bc5gChYPn2Z88\nEd+9cd+8q7ezWq0aAFBT9w/9AQCAfzyCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/T/0B/jH8l/+w79fZebGx9PwTK+f\n+3+pc/tGeGZvtJHa9faFYWruk1/+LDzznR/+PLVrbzILz/R6ybPvdFJzg7X18MylKzupXec34t/t\n83eupHb9+be+Hp6Zz+LXq7XWnu0fpeYGWxfDM+9+8NvUrr/97g/jQ8nnwNogN3dhMAjPDPuL1K5p\n4lrPZ7nfWFstU2NrvbXwzMkq/rxvrbUXp/F46eZ+Lu073/+75EH+P7t/3z8AAPzTJegBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+te3P84NddfxJuT\nBv1UUV67v5qEZ94f5yqQ3v7iK6m55TT+Ga/t5NraNlLfLXf22fa6k0n8PPZ3X6R2HXXiTWOT03Fq\n15e/+o3wzOzkNLXr2fPceVxbjzc3LqcHqV0ba/H7atlyrWtXt86l5r70ymvhmadP7qd2jceH4Zmj\no1xLYevGW/laa22tPw/P3Lx+IbVrNrwanvngV/dSu86CN3oAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZUpuPT9dScyfj/fDMsJMr92iLeKFCtzNMrXr2\n28epuZ88+Cw88+snudKS1SReSpEtp1lfX0/NzebxopnWzf0/vb4Rv4f3xrlilR+983545sblXCHI\nZJ67ZpkCo7XkE24wSHzG3NG3L7z6amruc3fuhme2t0apXY8e3gvPLGe55+K5izdSc4tBvPRotJYr\n3rm5Ey8i+rSXO/uz4I0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLLtdeNeriFrtxtvJ+ssJqldl/vx4z93/mJq1+lxvJWvtdb2DuPf7eB0ltq1\nSpz9YpFok2ut9ZKfsZ/533gWb11rrbXjafzsz61yu370i1+GZ15/7bXUrjdevZOa6w/j7V+f+1yu\nGe54OQjPPH74NLXr4HCcmmvrm+GRP/6zt1Orfv7j74VnxvN4G2VrrR3Oci1vz4/jz8ZL41zD3q3e\nYXjm9Cjb2vj780YPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAorW2qz1tlNzd0YxYsYtlu8AKO11i5d3AjPfLyKlym01trmxjI1t9aJl6SMOrnbara5Fp+Z\n58ppTie5IqJF4n/jjVGupGO4Fr+vrt++kdp186Xb4ZlnR7lCkEcHuRKXb3zj6+GZ3cePUrv+9b/5\nVnjmf/z3v07t+uEP/i41d+dLXw3PfPvtr6V2fXj/o/DMx9//cWrX/nQrNXc0jz/jvvjP42fYWmvj\n2YvwzM7OemrXWfBGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNAD\nQGGCHgAKE/QAUFjZ9rrhZu6rvbJ1NTzz8iq368Iw0Wa0/1lq12g73gzXWmvHw5PwzHKwSO364z+K\nN0lduxq/Xq219tEHH6TmPv3kfnim28u1G67m8Xa49W7u7P/kG/Gzfxq/NVprrf3oe99Nzb333p3w\nzGKc/JCbF8Mje8e5RsSjWe5964OHz8Mzx8teatfxPP4Zn+zlzmOyfi419/m7r4Rntq/dTO16+jx+\n9t/+9lupXWfBGz0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0BhZdvrjqa5xrALvc3wzOzZi9SuT/fiTWh/+uU3UrvG0+PU3K1lfGZ9tErt+uZ2/Ozf\nvLKT2nWyzH3GZ2vxFsCT/dz9sZjGZ/rTw9Suu598HJ7Z2Jundl26sp2am/39z8Iz2ebAH/7q3fDM\new8epHadznMtb/c/iTdZPnn+NLXr61/5Znjm7vbt1K7/9F//W2puOn4UnvnJj5+ldj1+/GF45qt/\nkXt2nwVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nsLKlNld666m5W60Xnjl/fiu16+cv4qUULyb7qV13r99Izf3bJy+HZwYHuQKdy+/Hz2Ptw4epXYvl\nLDX3uU58ZrBIDLXWuv34Pbzo5EpcJj/6aXjmQrKMZbkTLy9qrbXFPNGwdLBI7TrfOxeemRzn7vtL\n8UdOa6210Wocnjl49NvUrltffD08s7WZewZ//dVbqbkn+/EWqEdHJ6ldJye74ZmP3n8/tesseKMH\ngMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGx7\n3Rtbo9Tc5vNn4ZleN9Gq1Vp7/aWXwjOHj5+mdrVVrkHtVmcVnhkNc7t6iUaozjL++VprLd5z9Q8m\n3cT/xsO11K7BKv7d+pmGt9baoBtv85tt5WrXVie51rv5JH4ei5a7F69143fItzdyrXzTzjA1t7h5\nLTyzfu9eatdJ5iMmWz3feuO11NyNk/g1uzGbp3a9/urN8MxrO/FGxLPijR4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21Gb3wUepuck8XoIx7uWKRE4u\nxEsONk7i5SOttXb67oepuUVvEZ6Zb+Zuq24vXkqxlixx6bT11Nw8UQ60WOY+42owiM+kNuXm+ldf\nSe3a2su9X5wmLtn07sXUrovzo/DM5mmuKmm+lytWOXqyH545efD91K6H//sX4Znzb72e2vX8Ua64\nazq6FJ6Zj1Or2snzF+GZg0G2Suv3540eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdc+P9lJznx6fhmfmy1z71LBzPTwzuriT2vV8fJiau95b\nC89snOb+f1wcxJv5JtNcm1/byZ3j5uuvhWdOE01orbV29OwgPLO2jLfrtdZabzIJz0ye5u6ptpZr\nlOtsx9se+51cn9/yIP4c2Hgr1+bXhvHv1Vproyfx6rXj+/dTu/Z+/UF4ZvnJ49SurUtbqbnd7XhL\n5PNHud/mwyefhWdeHt5I7ToL3ugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nMEEPAIUJegAoTNADQGGCHgAKK9te9+I03j7VWmuPTuJtRrOD49SunWtXwjOr21dTu9Yu5hqh1g7i\nzXz9B09Tu6ZHJ+GZoxZvrGqttcW5jdTc4O6d8Ey/s0jt2tyOn8fsN5+kds0SLYCn3Vxz4NafvZma\nO9l7Fh9679epXW2eeAd6mPh8rbXJMte0Obh+Mzxz/V9+M7VrbaMXntn9zYepXdsn8V2ttXbhbrxp\n85NHuYa9jV68FXEwGKZ2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLKlNrdvv5Sa6358PzyzMU6taotpvBhhrTNI7XpxfJCa+8Gnn4Vnbp4epna9\n0eIHOUmUsbTW2vh+/Dq31tr0p7+K72rx69xaa51bt8Izp69fT+06mY/CM2+/miunOe6eS82NH9wL\nzwz3c+VW8/PxApLpJ8lCoce5UqzB1SfhmZNruVKswaUL4ZmLf/HV1K69Tx+m5rZ34mU4Xz13N7Xr\nb/7Xi/DM2na8xOyseKMHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAorGx73fWb11Jzh/efhWdGFzupXa2zFh4ZdHO7Hj57npr7z7/4P+GZL1zOtZP9\nx/XN8Mwo+a/q6vgoNbf7Try9bvdKvPmrtdY+msRbzabJprybr98Mz9y5mPte04ePU3PnEq1mneU0\ntasdxn9na92N1KqD8UlqbvHRR+GZ1YNHqV0vtuLPqs0v5BpEb778amru9FH8vroyij9zWmvtK196\nLTxz++XceZwFb/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAU\nJugBoLCypTb7ixepuf5qPzwz6OeOcdqLF5DszcepXbvjXNnJfBX/bgeDXLnH/cEoPLO9mqd2Tbu5\nudVqEp7ZX+ZKSz57Ei+1Od9dT+16kbhkf3X/r1K7vnDrVmru1Uvx73Z57Xpq1/G9++GZxTh+vVpr\nbbXI3YsvXjxN7Mo9B6br8VKb2X68IKy11qa/fD81N0oUOk3WB6ldd998Kzwze/Db1K6z4I0eAAoT\n9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdcPV\nMjXXX87CMzvdXAPStBdvrerPpqldJ6e587h15Up45qWXb6d23T9KNPOtcm1cw2RrVWce/8lMl/HG\nu9Zau3F5JzzTzxWhtYOnj8Izq91cK9+D57mWt/3RMDxzZxL/PbfWWvdZvL2ujXOH353n3rfG8/g5\nnixyz49VohVxNO6kdj28/1lqbtSJ7zue567Z9iQ+t/P266ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQ\nA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGypzcZ4lJp7ML8QnrnaPU3tujjeC8/0\nnzxM7ZofvkjNffHNl8Mzd77w+dSu3V+8F5650emldrVBrgxnsIr/b7xxlCtx6bf4ZxyNNlK7fvPh\nvfDMznHuPeGVz11KzX02jBfUPP4g93vZONwNz3TmuXuqs8jdw6eJUqxpN3fNpsfxXbuLw9Su0eh8\nau5wGi+POp7krtnu/cfhmf6d66ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeA\nwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+v2j+NNV6219t39eEvT/HJqVfvWchqe2XjyKLVrfXaS\nmvvK174dnrl5+7XUru/86J3wzP4k1xy46Ofuj1miLW9j1UntOv0sfq17l3LNcK9c3AnPnC72U7v6\nm8PU3Nt/+vXwzG680Owf5n7yJDwzWeaa0Jb9tdTcOHFfbW4mH1Ybm+GR8TDXyre8fDE1d9ri+x49\njbcUttba/t6z8MyLX7+f2vWXqanf5Y0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtddODB6m5D54/Ds+MZ7k2ru2X4o1hXx7kWte2+vFWvtZa\ne/n27fDM+XO5BrXJIt7mNzmJz7TW2nCwSM2druL7ht3c/TGcxq/ZeDfXxtXtxx8Fy16ure3x81wD\n44t3fxWeGa3nGtQO18/FZzZGqV2Tc1upuePj4/DMaCf329ydxlsiD+e531h3Nk7NPXx0FN+1Hm/l\na621g1n8ObB5kGt7PAve6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhQl6AChM0ANAYWVLbf7V3VxZwdPdeJnFjz8+Se36m3vxkoONV3Lfa3RuLTW31YsXdcwO4wUYrbW2\n6MRLMI4nuV3rvdytv+gl/jfu5P6fXnbjc7vH8WKP1lpbncYLdIbHubOf7eWKiFYffhKeGSXfZaaj\n8+GZd+aT1K57z56k5taX8ZnhMlcYM1iP/146s05q1+lerpjpeBUvB+qfG6R2LQbx73b34nZq11nw\nRg8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY\n2fa612/mvtq/G90Jz9xeu5/a9T/fizeN/e29WWrXH929mZo7+vDj8Mxe8v/H3jJex7U3zTUHXhnF\nm65aa22x6oVnZsvcNXu6ip/Hs1G8fbG11k778fa6rU7uN7Z5IXf2y2n8M7bnB6lda2vxlsjPTnPN\ncM8Xq9Tc9UG8eW20mbs/tjbj57Ea59oNn01z59jvxZ8Fvd3c8+NLq2F45txh7jlwFrzRA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCypbaTJJlJ5fWO+GZ\nP3l9J7Xr2XG8tOQn9/dTu959/CI19/lEUcd0mLutVsv4/52Hp5Pcrkm8lKK11gbr8e+2WuZKS1pi\nbmNtPbXqcBUvIDm4cy216/Jbb6TmevGfS3vnr7+X2nU7cV+9dPFKalebTFNj6/34gezPcoUxx8/j\nz9PryYKlmzuXU3PDbvy3OdjNPU/vHsYLyW5vb6d2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoA\nKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIWVba/r9HJfrTOPt1bd2M41hv2Lly+EZw6m\n8Zax1lq7t5dr8zvpxdv8rt6+ndrVG47CM6fzXDPc6eFhaq4/W4RnhoON1K743dHa/PHT1K7zi3l4\nZnKQu6d2Z4kautba9sWL8ZlO7l1mcBr/brc2N1O7hsn3rc7mWnxmkPuM3aN4w961fvz33FpriQLR\n1lpr3Un8t3mSfA5c6MXvj1fv5HLiLHijB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQ\nmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91qlatAWi0T7WTLeONda629eSl+/E9vnEvtOp7kPuN8\nHG/L27l8JbVr/Vy8r21vmWuvm01nqbl5Ym7SyzUOdju98Mz55L/umV6t6cF+btlp7jxWj56EZ15q\nuefAoBdv89sa587jai/Xbvgi0Ui5thVvAGytteUsfmPNT/ZSuw4muVbERHldW06OU7tuvHk1PPPy\nndxz8Sx4oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhZUttVl2cv/DLFq8SKTNcwUpF/rxwo2v3N5J7Xp+uJuamz5+GJ6ZHeeKIoab8XKP0+R1nq1yc91l\n/FovZom2jdZaZxG/P+bJ85gOMuUv8eKX1lrrzHPnsegN40PdXKnNYh7/bqtkWc/6YpCaW82m4ZlH\n67mimdla/OyXa6lVbbCZO4+Tk/h5DFfL1K4rd66HZ9b7ifv3jHijB4DCBD0AFCboAaAwQQ8AhQl6\nAChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91wYzM111sfhWeme0epXZlW\ns5vb8c/XWmv/bD/XrPXu3uPwzKMHn6R2HYwPwjNHy1z71Gk39z/uYLkKz8xXuba27ir+8zzu5Nra\nTlbxuX7yPWE5yV2z5SR+D3eS7XUtcZ1P+7nrvEw05bXW2nHmM65NUrtaN/7d1ge5+rrlIt5C11pr\nm8v4d3vt2lZq18Vh/OxPnueaA3Of8Hd5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QA\nUJigB4DCBD0AFCboAaAwQQ8AhZUttWndXmqs0xmEZ/obqVXttDsLzwwSZQqttXbnRq4M5+PP4gUT\n08lxatdiGd+1N88VYDzr5G79rV78vuqscteskyio2c/1xbRH03hpSbeTe0/oJQp0srJvMoMWv86P\nl/Hfc2ut7bdcGc5R4lrfSpb8bCcKuHq7h6ld1/rrqbmv3b4ennn1du7hPRrHi8wmybIepTYAwP+X\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhdVtr1vm\n/oeZjE/CM9k2rk6iSWo1zTVkndvcTM3tnI83Lu0+fZLadfgoPrffy13nHySbxi4miujOJxoRW2tt\nM9FeN+vmmvIO5vG502TrWra7rteNX+thom2wtdZGqU+Z29Xv5CoHR4lrvZzNU7umi/h5bCTvjwvn\ncp+xzQ7CI0cvcmd/cD7+m+7Mc8+cndTU7/JGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZ9rrFMtfitUrMdZINasP+MDyzGucakFruONrVzfhn\n/Ok7f5/a9fzB0/DMvJO7hZ8mO9QO5vE2v9Ei2U6W+IhryXtxNYxf526iTa611jqJVr7WWuv3441h\ni1WynWwR/53N57m2tlXyMw4zx59sr1sm7qtuP/fQWbbcM27vaC8801vlzmOtuxWe6Sz/cHHrjR4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21KY7iBdg\ntNbaINHD0EkWxnR6ieNf5IozFsdHqbkbW6PwzOVB7jMOTsfhmfPLXEHKaSf3P243MTfv50pLjpfx\nuXHyXmyJEpfePLeskywU6iYKhVarZLlVJ372uW/V2qDTy80lnh8byfv+XGJss5N8DuTGWmvxwcn4\nOLUp8zgddePP0rPijR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJig\nB4DCBD0AFCboAaCwuu11/dxX660S//uscu1kLdVel2vl63dz3VrnOvHGsD9762Zq1/5JfNfPPnmW\n2vVsMk/NnS7jbWiTZK/ZMnF/LJP/uy8S36ubrG3sJGveut1sNV9cL9Hy1k9+vI1u7lk16safBVv9\n3OFvdePPuMvJdBklb5BBi/+mh8l7arWI7zpNtHOeFW/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAo\nTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaCwsqU2bbieHIyXFXRWyTaLRPHOfD5LrVomL3WmvOHG\nKLWq/eWXb4Vnrg1yhUIfPD5IzT0+jp//i3mupON02QvPTJK34rwTv86rRPFLa611e/Hv1VprvcRc\nsj+nDRIlP/1kt9VmptyqtbaWOP+1Tu5Dnu8twjMXkwU6m73cfbU+iJ9jP3crttks/hw46cTP8Kx4\noweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6ACis\ns8o2rwEA/+R5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAw\nQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM\n0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/xfkBwlHN40TWAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe72a448da0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "map_one_hot_encode = [[1,0,0,0,0,0,0,0,0,0],\n",
    "                      [0,1,0,0,0,0,0,0,0,0],\n",
    "                      [0,0,1,0,0,0,0,0,0,0],\n",
    "                      [0,0,0,1,0,0,0,0,0,0],\n",
    "                      [0,0,0,0,1,0,0,0,0,0],\n",
    "                      [0,0,0,0,0,1,0,0,0,0],\n",
    "                      [0,0,0,0,0,0,1,0,0,0],\n",
    "                      [0,0,0,0,0,0,0,1,0,0],\n",
    "                      [0,0,0,0,0,0,0,0,1,0],\n",
    "                      [0,0,0,0,0,0,0,0,0,1]]\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    encoded_val = np.zeros((len(x),10), dtype=np.int)\n",
    "    for idx, val in enumerate(x):\n",
    "        encoded_val[idx] = map_one_hot_encode[val]\n",
    "    return encoded_val\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54901961,  0.49019608,  0.45098039],\n",
       "       [ 0.57254902,  0.50980392,  0.47843137],\n",
       "       [ 0.56078431,  0.49803922,  0.47843137],\n",
       "       [ 0.56078431,  0.49411765,  0.48235294],\n",
       "       [ 0.61176471,  0.54509804,  0.54117647],\n",
       "       [ 0.63137255,  0.56078431,  0.5372549 ],\n",
       "       [ 0.60784314,  0.5372549 ,  0.47843137],\n",
       "       [ 0.58431373,  0.51372549,  0.43921569],\n",
       "       [ 0.60392157,  0.53333333,  0.45882353],\n",
       "       [ 0.62352941,  0.54901961,  0.50196078],\n",
       "       [ 0.65490196,  0.57254902,  0.56078431],\n",
       "       [ 0.67058824,  0.58039216,  0.6       ],\n",
       "       [ 0.64313725,  0.54117647,  0.56470588],\n",
       "       [ 0.63921569,  0.54117647,  0.54509804],\n",
       "       [ 0.63137255,  0.54509804,  0.52156863],\n",
       "       [ 0.61568627,  0.54117647,  0.50588235],\n",
       "       [ 0.65098039,  0.58431373,  0.55686275],\n",
       "       [ 0.61960784,  0.55294118,  0.54509804],\n",
       "       [ 0.56862745,  0.49411765,  0.49411765],\n",
       "       [ 0.58431373,  0.50196078,  0.50980392],\n",
       "       [ 0.59215686,  0.50588235,  0.5254902 ],\n",
       "       [ 0.62352941,  0.5372549 ,  0.56470588],\n",
       "       [ 0.61960784,  0.5372549 ,  0.54901961],\n",
       "       [ 0.62352941,  0.54509804,  0.5254902 ],\n",
       "       [ 0.65882353,  0.58431373,  0.54117647],\n",
       "       [ 0.6627451 ,  0.58823529,  0.54901961],\n",
       "       [ 0.63921569,  0.56470588,  0.5254902 ],\n",
       "       [ 0.64705882,  0.56862745,  0.52156863],\n",
       "       [ 0.64705882,  0.55686275,  0.50588235],\n",
       "       [ 0.66666667,  0.56862745,  0.51372549],\n",
       "       [ 0.69019608,  0.58823529,  0.5254902 ],\n",
       "       [ 0.66666667,  0.57647059,  0.52156863]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking data:\n",
    "#Features is a 4D shape valid_features[batch,image,pixel,channel]\n",
    "#Get First Image in first batch\n",
    "valid_features[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54901961,  0.57254902,  0.56078431,  0.56078431,  0.61176471,\n",
       "        0.63137255,  0.60784314,  0.58431373,  0.60392157,  0.62352941,\n",
       "        0.65490196,  0.67058824,  0.64313725,  0.63921569,  0.63137255,\n",
       "        0.61568627,  0.65098039,  0.61960784,  0.56862745,  0.58431373,\n",
       "        0.59215686,  0.62352941,  0.61960784,  0.62352941,  0.65882353,\n",
       "        0.6627451 ,  0.63921569,  0.64705882,  0.64705882,  0.66666667,\n",
       "        0.69019608,  0.66666667])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all pixel values for the first image for the first channel\n",
    "valid_features[0,0,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None,image_shape[0], image_shape[1], image_shape[2]), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None,n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None), name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    # Weight and bias\n",
    "    input_channels=x_tensor.shape[3].value\n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],input_channels,conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "   \n",
    "    # Apply convolution\n",
    "    conv_strides_size = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,filter_weights,strides=conv_strides_size, padding='SAME')\n",
    "    \n",
    "    # Apply bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # Apply pooling\n",
    "    pool_filter_size = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    pool_strides_size = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, pool_filter_size , pool_strides_size , padding='SAME')\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    flattened_image_size = x_tensor.shape[1].value * x_tensor.shape[2].value * x_tensor.shape[3].value\n",
    "    return tf.reshape(x_tensor, [-1, flattened_image_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "        \n",
    "    # Weight and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=1/np.sqrt(x_tensor.shape[1].value)))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fully_conn_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "    return fully_conn_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Weight and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=1/np.sqrt(x_tensor.shape[1].value)))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    # Output Layer\n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    x_tensor = conv2d_maxpool(x, 50, (16,16), (2,2), (8,8) ,(8,8) )\n",
    "    \n",
    "    x_tensor = conv2d_maxpool(x_tensor, 20, (4,4), (2,2), (2,2) ,(2,2) )\n",
    "\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    x_tensor = flatten(x_tensor)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    x_tensor = fully_conn(x_tensor, 16)\n",
    "    \n",
    "    x_tensor = fully_conn(x_tensor, 8)\n",
    "\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    x_tensor = output(x_tensor, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_loss, batch_acc = sess.run([cost, accuracy], feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    print(\"Batch Loss:\", batch_loss, \"Batch Accuracy:\", batch_acc)\n",
    "    \n",
    "    valid_loss, valid_acc = sess.run([cost, accuracy], feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    print(\"Validation Loss:\", valid_loss, \"Validation Accuracy:\", valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Batch Loss: 11.1231 Batch Accuracy: 0.125\n",
      "Validation Loss: 10.3791 Validation Accuracy: 0.133\n",
      "Epoch  2, CIFAR-10 Batch 1:  Batch Loss: 6.10537 Batch Accuracy: 0.175\n",
      "Validation Loss: 5.64085 Validation Accuracy: 0.1372\n",
      "Epoch  3, CIFAR-10 Batch 1:  Batch Loss: 4.06806 Batch Accuracy: 0.15\n",
      "Validation Loss: 3.74686 Validation Accuracy: 0.156\n",
      "Epoch  4, CIFAR-10 Batch 1:  Batch Loss: 3.09807 Batch Accuracy: 0.125\n",
      "Validation Loss: 2.91997 Validation Accuracy: 0.1682\n",
      "Epoch  5, CIFAR-10 Batch 1:  Batch Loss: 2.59592 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.47451 Validation Accuracy: 0.1808\n",
      "Epoch  6, CIFAR-10 Batch 1:  Batch Loss: 2.3697 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.2751 Validation Accuracy: 0.191\n",
      "Epoch  7, CIFAR-10 Batch 1:  Batch Loss: 2.27501 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.18597 Validation Accuracy: 0.2\n",
      "Epoch  8, CIFAR-10 Batch 1:  Batch Loss: 2.22796 Batch Accuracy: 0.225\n",
      "Validation Loss: 2.13618 Validation Accuracy: 0.211\n",
      "Epoch  9, CIFAR-10 Batch 1:  Batch Loss: 2.1997 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.10257 Validation Accuracy: 0.2198\n",
      "Epoch 10, CIFAR-10 Batch 1:  Batch Loss: 2.17715 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.07631 Validation Accuracy: 0.2212\n",
      "Epoch 11, CIFAR-10 Batch 1:  Batch Loss: 2.16064 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.05682 Validation Accuracy: 0.229\n",
      "Epoch 12, CIFAR-10 Batch 1:  Batch Loss: 2.14441 Batch Accuracy: 0.175\n",
      "Validation Loss: 2.04143 Validation Accuracy: 0.2334\n",
      "Epoch 13, CIFAR-10 Batch 1:  Batch Loss: 2.12973 Batch Accuracy: 0.15\n",
      "Validation Loss: 2.02816 Validation Accuracy: 0.2406\n",
      "Epoch 14, CIFAR-10 Batch 1:  Batch Loss: 2.11513 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.01656 Validation Accuracy: 0.2438\n",
      "Epoch 15, CIFAR-10 Batch 1:  Batch Loss: 2.10192 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.00686 Validation Accuracy: 0.2458\n",
      "Epoch 16, CIFAR-10 Batch 1:  Batch Loss: 2.08967 Batch Accuracy: 0.2\n",
      "Validation Loss: 1.99858 Validation Accuracy: 0.2458\n",
      "Epoch 17, CIFAR-10 Batch 1:  Batch Loss: 2.07722 Batch Accuracy: 0.2\n",
      "Validation Loss: 1.99185 Validation Accuracy: 0.249\n",
      "Epoch 18, CIFAR-10 Batch 1:  Batch Loss: 2.06431 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.98597 Validation Accuracy: 0.2494\n",
      "Epoch 19, CIFAR-10 Batch 1:  Batch Loss: 2.05583 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.9807 Validation Accuracy: 0.2502\n",
      "Epoch 20, CIFAR-10 Batch 1:  Batch Loss: 2.04962 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.97579 Validation Accuracy: 0.2534\n",
      "Epoch 21, CIFAR-10 Batch 1:  Batch Loss: 2.04486 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.97076 Validation Accuracy: 0.2548\n",
      "Epoch 22, CIFAR-10 Batch 1:  Batch Loss: 2.04058 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.96493 Validation Accuracy: 0.2592\n",
      "Epoch 23, CIFAR-10 Batch 1:  Batch Loss: 2.03358 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.95718 Validation Accuracy: 0.2654\n",
      "Epoch 24, CIFAR-10 Batch 1:  Batch Loss: 2.02513 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.94702 Validation Accuracy: 0.2656\n",
      "Epoch 25, CIFAR-10 Batch 1:  Batch Loss: 2.01743 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.93534 Validation Accuracy: 0.2686\n",
      "Epoch 26, CIFAR-10 Batch 1:  Batch Loss: 2.00935 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.92466 Validation Accuracy: 0.2748\n",
      "Epoch 27, CIFAR-10 Batch 1:  Batch Loss: 2.00556 Batch Accuracy: 0.2\n",
      "Validation Loss: 1.91521 Validation Accuracy: 0.2782\n",
      "Epoch 28, CIFAR-10 Batch 1:  Batch Loss: 2.00233 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.90638 Validation Accuracy: 0.2806\n",
      "Epoch 29, CIFAR-10 Batch 1:  Batch Loss: 1.99768 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.89935 Validation Accuracy: 0.2836\n",
      "Epoch 30, CIFAR-10 Batch 1:  Batch Loss: 1.99421 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.89231 Validation Accuracy: 0.2834\n",
      "Epoch 31, CIFAR-10 Batch 1:  Batch Loss: 1.99107 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.88506 Validation Accuracy: 0.2864\n",
      "Epoch 32, CIFAR-10 Batch 1:  Batch Loss: 1.98752 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.87878 Validation Accuracy: 0.2918\n",
      "Epoch 33, CIFAR-10 Batch 1:  Batch Loss: 1.98477 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.87129 Validation Accuracy: 0.294\n",
      "Epoch 34, CIFAR-10 Batch 1:  Batch Loss: 1.98245 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.86527 Validation Accuracy: 0.2978\n",
      "Epoch 35, CIFAR-10 Batch 1:  Batch Loss: 1.97929 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.85913 Validation Accuracy: 0.3004\n",
      "Epoch 36, CIFAR-10 Batch 1:  Batch Loss: 1.9773 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.85268 Validation Accuracy: 0.3004\n",
      "Epoch 37, CIFAR-10 Batch 1:  Batch Loss: 1.97448 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.84805 Validation Accuracy: 0.3076\n",
      "Epoch 38, CIFAR-10 Batch 1:  Batch Loss: 1.96941 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.84219 Validation Accuracy: 0.311\n",
      "Epoch 39, CIFAR-10 Batch 1:  Batch Loss: 1.96439 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.83837 Validation Accuracy: 0.3122\n",
      "Epoch 40, CIFAR-10 Batch 1:  Batch Loss: 1.95412 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.83282 Validation Accuracy: 0.3148\n",
      "Epoch 41, CIFAR-10 Batch 1:  Batch Loss: 1.94639 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.82933 Validation Accuracy: 0.3184\n",
      "Epoch 42, CIFAR-10 Batch 1:  Batch Loss: 1.93667 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.82377 Validation Accuracy: 0.321\n",
      "Epoch 43, CIFAR-10 Batch 1:  Batch Loss: 1.92962 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.82034 Validation Accuracy: 0.3222\n",
      "Epoch 44, CIFAR-10 Batch 1:  Batch Loss: 1.91937 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.81483 Validation Accuracy: 0.3258\n",
      "Epoch 45, CIFAR-10 Batch 1:  Batch Loss: 1.91003 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.81161 Validation Accuracy: 0.3298\n",
      "Epoch 46, CIFAR-10 Batch 1:  Batch Loss: 1.89918 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.8067 Validation Accuracy: 0.3314\n",
      "Epoch 47, CIFAR-10 Batch 1:  Batch Loss: 1.89154 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.80316 Validation Accuracy: 0.3338\n",
      "Epoch 48, CIFAR-10 Batch 1:  Batch Loss: 1.88276 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.79894 Validation Accuracy: 0.3366\n",
      "Epoch 49, CIFAR-10 Batch 1:  Batch Loss: 1.87163 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.79401 Validation Accuracy: 0.339\n",
      "Epoch 50, CIFAR-10 Batch 1:  Batch Loss: 1.86193 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.78988 Validation Accuracy: 0.3414\n",
      "Epoch 51, CIFAR-10 Batch 1:  Batch Loss: 1.84798 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.7841 Validation Accuracy: 0.343\n",
      "Epoch 52, CIFAR-10 Batch 1:  Batch Loss: 1.83252 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.78042 Validation Accuracy: 0.3434\n",
      "Epoch 53, CIFAR-10 Batch 1:  Batch Loss: 1.82108 Batch Accuracy: 0.35\n",
      "Validation Loss: 1.77617 Validation Accuracy: 0.346\n",
      "Epoch 54, CIFAR-10 Batch 1:  Batch Loss: 1.80298 Batch Accuracy: 0.35\n",
      "Validation Loss: 1.7706 Validation Accuracy: 0.349\n",
      "Epoch 55, CIFAR-10 Batch 1:  Batch Loss: 1.78666 Batch Accuracy: 0.35\n",
      "Validation Loss: 1.76677 Validation Accuracy: 0.3494\n",
      "Epoch 56, CIFAR-10 Batch 1:  Batch Loss: 1.77405 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.76269 Validation Accuracy: 0.3528\n",
      "Epoch 57, CIFAR-10 Batch 1:  Batch Loss: 1.7562 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.75622 Validation Accuracy: 0.3578\n",
      "Epoch 58, CIFAR-10 Batch 1:  Batch Loss: 1.73836 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.75148 Validation Accuracy: 0.3602\n",
      "Epoch 59, CIFAR-10 Batch 1:  Batch Loss: 1.72392 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.74609 Validation Accuracy: 0.3608\n",
      "Epoch 60, CIFAR-10 Batch 1:  Batch Loss: 1.70548 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.7413 Validation Accuracy: 0.3646\n",
      "Epoch 61, CIFAR-10 Batch 1:  Batch Loss: 1.69148 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.73668 Validation Accuracy: 0.3648\n",
      "Epoch 62, CIFAR-10 Batch 1:  Batch Loss: 1.67356 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.73222 Validation Accuracy: 0.3676\n",
      "Epoch 63, CIFAR-10 Batch 1:  Batch Loss: 1.65775 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.72741 Validation Accuracy: 0.3704\n",
      "Epoch 64, CIFAR-10 Batch 1:  Batch Loss: 1.6425 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.72375 Validation Accuracy: 0.3716\n",
      "Epoch 65, CIFAR-10 Batch 1:  Batch Loss: 1.628 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.71949 Validation Accuracy: 0.3738\n",
      "Epoch 66, CIFAR-10 Batch 1:  Batch Loss: 1.61009 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.7135 Validation Accuracy: 0.3776\n",
      "Epoch 67, CIFAR-10 Batch 1:  Batch Loss: 1.59661 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.71022 Validation Accuracy: 0.3808\n",
      "Epoch 68, CIFAR-10 Batch 1:  Batch Loss: 1.58061 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.70439 Validation Accuracy: 0.3822\n",
      "Epoch 69, CIFAR-10 Batch 1:  Batch Loss: 1.56763 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.70164 Validation Accuracy: 0.3834\n",
      "Epoch 70, CIFAR-10 Batch 1:  Batch Loss: 1.55485 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.69687 Validation Accuracy: 0.3856\n",
      "Epoch 71, CIFAR-10 Batch 1:  Batch Loss: 1.53964 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.69408 Validation Accuracy: 0.3886\n",
      "Epoch 72, CIFAR-10 Batch 1:  Batch Loss: 1.52861 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.68908 Validation Accuracy: 0.3898\n",
      "Epoch 73, CIFAR-10 Batch 1:  Batch Loss: 1.51217 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.68576 Validation Accuracy: 0.3906\n",
      "Epoch 74, CIFAR-10 Batch 1:  Batch Loss: 1.502 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.68144 Validation Accuracy: 0.3936\n",
      "Epoch 75, CIFAR-10 Batch 1:  Batch Loss: 1.48752 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.67969 Validation Accuracy: 0.3948\n",
      "Epoch 76, CIFAR-10 Batch 1:  Batch Loss: 1.47432 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.67553 Validation Accuracy: 0.3962\n",
      "Epoch 77, CIFAR-10 Batch 1:  Batch Loss: 1.45794 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.67199 Validation Accuracy: 0.3974\n",
      "Epoch 78, CIFAR-10 Batch 1:  Batch Loss: 1.44196 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.67287 Validation Accuracy: 0.4006\n",
      "Epoch 79, CIFAR-10 Batch 1:  Batch Loss: 1.42771 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.66814 Validation Accuracy: 0.4028\n",
      "Epoch 80, CIFAR-10 Batch 1:  Batch Loss: 1.41869 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.66446 Validation Accuracy: 0.4036\n",
      "Epoch 81, CIFAR-10 Batch 1:  Batch Loss: 1.40516 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.66192 Validation Accuracy: 0.4054\n",
      "Epoch 82, CIFAR-10 Batch 1:  Batch Loss: 1.39507 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.66027 Validation Accuracy: 0.405\n",
      "Epoch 83, CIFAR-10 Batch 1:  Batch Loss: 1.38352 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.65654 Validation Accuracy: 0.4098\n",
      "Epoch 84, CIFAR-10 Batch 1:  Batch Loss: 1.37454 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.65601 Validation Accuracy: 0.4104\n",
      "Epoch 85, CIFAR-10 Batch 1:  Batch Loss: 1.3648 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.65297 Validation Accuracy: 0.4116\n",
      "Epoch 86, CIFAR-10 Batch 1:  Batch Loss: 1.35528 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.65149 Validation Accuracy: 0.4126\n",
      "Epoch 87, CIFAR-10 Batch 1:  Batch Loss: 1.34388 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.65105 Validation Accuracy: 0.414\n",
      "Epoch 88, CIFAR-10 Batch 1:  Batch Loss: 1.33609 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.64894 Validation Accuracy: 0.4148\n",
      "Epoch 89, CIFAR-10 Batch 1:  Batch Loss: 1.32595 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.64859 Validation Accuracy: 0.4142\n",
      "Epoch 90, CIFAR-10 Batch 1:  Batch Loss: 1.32064 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.64423 Validation Accuracy: 0.4174\n",
      "Epoch 91, CIFAR-10 Batch 1:  Batch Loss: 1.30927 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.64675 Validation Accuracy: 0.4176\n",
      "Epoch 92, CIFAR-10 Batch 1:  Batch Loss: 1.30255 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.6441 Validation Accuracy: 0.4194\n",
      "Epoch 93, CIFAR-10 Batch 1:  Batch Loss: 1.29357 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.64298 Validation Accuracy: 0.4192\n",
      "Epoch 94, CIFAR-10 Batch 1:  Batch Loss: 1.28823 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.64015 Validation Accuracy: 0.4186\n",
      "Epoch 95, CIFAR-10 Batch 1:  Batch Loss: 1.27801 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.64026 Validation Accuracy: 0.419\n",
      "Epoch 96, CIFAR-10 Batch 1:  Batch Loss: 1.26942 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.6389 Validation Accuracy: 0.4196\n",
      "Epoch 97, CIFAR-10 Batch 1:  Batch Loss: 1.26184 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.63772 Validation Accuracy: 0.4212\n",
      "Epoch 98, CIFAR-10 Batch 1:  Batch Loss: 1.25493 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.63833 Validation Accuracy: 0.4212\n",
      "Epoch 99, CIFAR-10 Batch 1:  Batch Loss: 1.24624 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.63455 Validation Accuracy: 0.4226\n",
      "Epoch 100, CIFAR-10 Batch 1:  Batch Loss: 1.23685 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.6354 Validation Accuracy: 0.422\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Batch Loss: 8.20883 Batch Accuracy: 0.125\n",
      "Validation Loss: 6.05326 Validation Accuracy: 0.1244\n",
      "Epoch  1, CIFAR-10 Batch 2:  Batch Loss: 3.54376 Batch Accuracy: 0.075\n",
      "Validation Loss: 3.64822 Validation Accuracy: 0.121\n",
      "Epoch  1, CIFAR-10 Batch 3:  Batch Loss: 2.451 Batch Accuracy: 0.1\n",
      "Validation Loss: 2.8048 Validation Accuracy: 0.1286\n",
      "Epoch  1, CIFAR-10 Batch 4:  Batch Loss: 2.44384 Batch Accuracy: 0.15\n",
      "Validation Loss: 2.42902 Validation Accuracy: 0.133\n",
      "Epoch  1, CIFAR-10 Batch 5:  Batch Loss: 2.31914 Batch Accuracy: 0.175\n",
      "Validation Loss: 2.27828 Validation Accuracy: 0.1468\n",
      "Epoch  2, CIFAR-10 Batch 1:  Batch Loss: 2.31713 Batch Accuracy: 0.175\n",
      "Validation Loss: 2.21382 Validation Accuracy: 0.1618\n",
      "Epoch  2, CIFAR-10 Batch 2:  Batch Loss: 2.20187 Batch Accuracy: 0.15\n",
      "Validation Loss: 2.19208 Validation Accuracy: 0.16\n",
      "Epoch  2, CIFAR-10 Batch 3:  Batch Loss: 2.06342 Batch Accuracy: 0.225\n",
      "Validation Loss: 2.15491 Validation Accuracy: 0.1678\n",
      "Epoch  2, CIFAR-10 Batch 4:  Batch Loss: 2.08392 Batch Accuracy: 0.3\n",
      "Validation Loss: 2.10427 Validation Accuracy: 0.1904\n",
      "Epoch  2, CIFAR-10 Batch 5:  Batch Loss: 2.19486 Batch Accuracy: 0.125\n",
      "Validation Loss: 2.0787 Validation Accuracy: 0.1978\n",
      "Epoch  3, CIFAR-10 Batch 1:  Batch Loss: 2.31686 Batch Accuracy: 0.15\n",
      "Validation Loss: 2.0631 Validation Accuracy: 0.2048\n",
      "Epoch  3, CIFAR-10 Batch 2:  Batch Loss: 2.17808 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.06384 Validation Accuracy: 0.1972\n",
      "Epoch  3, CIFAR-10 Batch 3:  Batch Loss: 1.91844 Batch Accuracy: 0.325\n",
      "Validation Loss: 2.04696 Validation Accuracy: 0.203\n",
      "Epoch  3, CIFAR-10 Batch 4:  Batch Loss: 1.9805 Batch Accuracy: 0.25\n",
      "Validation Loss: 2.0214 Validation Accuracy: 0.2132\n",
      "Epoch  3, CIFAR-10 Batch 5:  Batch Loss: 2.14149 Batch Accuracy: 0.15\n",
      "Validation Loss: 2.00839 Validation Accuracy: 0.2216\n",
      "Epoch  4, CIFAR-10 Batch 1:  Batch Loss: 2.32516 Batch Accuracy: 0.15\n",
      "Validation Loss: 2.00167 Validation Accuracy: 0.222\n",
      "Epoch  4, CIFAR-10 Batch 2:  Batch Loss: 2.15138 Batch Accuracy: 0.2\n",
      "Validation Loss: 2.00345 Validation Accuracy: 0.216\n",
      "Epoch  4, CIFAR-10 Batch 3:  Batch Loss: 1.85022 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.99469 Validation Accuracy: 0.2208\n",
      "Epoch  4, CIFAR-10 Batch 4:  Batch Loss: 1.93484 Batch Accuracy: 0.2\n",
      "Validation Loss: 1.97177 Validation Accuracy: 0.2296\n",
      "Epoch  4, CIFAR-10 Batch 5:  Batch Loss: 2.09611 Batch Accuracy: 0.15\n",
      "Validation Loss: 1.9612 Validation Accuracy: 0.234\n",
      "Epoch  5, CIFAR-10 Batch 1:  Batch Loss: 2.3253 Batch Accuracy: 0.15\n",
      "Validation Loss: 1.9554 Validation Accuracy: 0.2392\n",
      "Epoch  5, CIFAR-10 Batch 2:  Batch Loss: 2.09229 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.95744 Validation Accuracy: 0.2372\n",
      "Epoch  5, CIFAR-10 Batch 3:  Batch Loss: 1.78927 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.94846 Validation Accuracy: 0.239\n",
      "Epoch  5, CIFAR-10 Batch 4:  Batch Loss: 1.88866 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.92845 Validation Accuracy: 0.2412\n",
      "Epoch  5, CIFAR-10 Batch 5:  Batch Loss: 2.04725 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.91793 Validation Accuracy: 0.2442\n",
      "Epoch  6, CIFAR-10 Batch 1:  Batch Loss: 2.32169 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.91175 Validation Accuracy: 0.2502\n",
      "Epoch  6, CIFAR-10 Batch 2:  Batch Loss: 2.05648 Batch Accuracy: 0.2\n",
      "Validation Loss: 1.9164 Validation Accuracy: 0.2534\n",
      "Epoch  6, CIFAR-10 Batch 3:  Batch Loss: 1.73362 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.9056 Validation Accuracy: 0.2552\n",
      "Epoch  6, CIFAR-10 Batch 4:  Batch Loss: 1.83706 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.89218 Validation Accuracy: 0.25\n",
      "Epoch  6, CIFAR-10 Batch 5:  Batch Loss: 1.99545 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.88169 Validation Accuracy: 0.2662\n",
      "Epoch  7, CIFAR-10 Batch 1:  Batch Loss: 2.31231 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.87485 Validation Accuracy: 0.2694\n",
      "Epoch  7, CIFAR-10 Batch 2:  Batch Loss: 2.02337 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.88284 Validation Accuracy: 0.2658\n",
      "Epoch  7, CIFAR-10 Batch 3:  Batch Loss: 1.6876 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.86746 Validation Accuracy: 0.2762\n",
      "Epoch  7, CIFAR-10 Batch 4:  Batch Loss: 1.80652 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.86157 Validation Accuracy: 0.266\n",
      "Epoch  7, CIFAR-10 Batch 5:  Batch Loss: 1.94846 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.85399 Validation Accuracy: 0.2782\n",
      "Epoch  8, CIFAR-10 Batch 1:  Batch Loss: 2.28553 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.84409 Validation Accuracy: 0.2866\n",
      "Epoch  8, CIFAR-10 Batch 2:  Batch Loss: 1.99456 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.85504 Validation Accuracy: 0.2748\n",
      "Epoch  8, CIFAR-10 Batch 3:  Batch Loss: 1.65531 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.8381 Validation Accuracy: 0.2922\n",
      "Epoch  8, CIFAR-10 Batch 4:  Batch Loss: 1.77541 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.83522 Validation Accuracy: 0.2814\n",
      "Epoch  8, CIFAR-10 Batch 5:  Batch Loss: 1.91365 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.82605 Validation Accuracy: 0.2936\n",
      "Epoch  9, CIFAR-10 Batch 1:  Batch Loss: 2.26402 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.81772 Validation Accuracy: 0.3018\n",
      "Epoch  9, CIFAR-10 Batch 2:  Batch Loss: 1.957 Batch Accuracy: 0.225\n",
      "Validation Loss: 1.83423 Validation Accuracy: 0.2818\n",
      "Epoch  9, CIFAR-10 Batch 3:  Batch Loss: 1.61974 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.81317 Validation Accuracy: 0.3052\n",
      "Epoch  9, CIFAR-10 Batch 4:  Batch Loss: 1.74532 Batch Accuracy: 0.35\n",
      "Validation Loss: 1.81027 Validation Accuracy: 0.2878\n",
      "Epoch  9, CIFAR-10 Batch 5:  Batch Loss: 1.89234 Batch Accuracy: 0.25\n",
      "Validation Loss: 1.79869 Validation Accuracy: 0.3116\n",
      "Epoch 10, CIFAR-10 Batch 1:  Batch Loss: 2.24994 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.79341 Validation Accuracy: 0.3118\n",
      "Epoch 10, CIFAR-10 Batch 2:  Batch Loss: 1.91637 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.80759 Validation Accuracy: 0.2938\n",
      "Epoch 10, CIFAR-10 Batch 3:  Batch Loss: 1.59099 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.79186 Validation Accuracy: 0.3134\n",
      "Epoch 10, CIFAR-10 Batch 4:  Batch Loss: 1.71667 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.78807 Validation Accuracy: 0.3012\n",
      "Epoch 10, CIFAR-10 Batch 5:  Batch Loss: 1.86881 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.77568 Validation Accuracy: 0.3234\n",
      "Epoch 11, CIFAR-10 Batch 1:  Batch Loss: 2.25008 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.77204 Validation Accuracy: 0.315\n",
      "Epoch 11, CIFAR-10 Batch 2:  Batch Loss: 1.88033 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.78593 Validation Accuracy: 0.3072\n",
      "Epoch 11, CIFAR-10 Batch 3:  Batch Loss: 1.56628 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.77255 Validation Accuracy: 0.321\n",
      "Epoch 11, CIFAR-10 Batch 4:  Batch Loss: 1.69456 Batch Accuracy: 0.35\n",
      "Validation Loss: 1.76639 Validation Accuracy: 0.3122\n",
      "Epoch 11, CIFAR-10 Batch 5:  Batch Loss: 1.8412 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.75434 Validation Accuracy: 0.3326\n",
      "Epoch 12, CIFAR-10 Batch 1:  Batch Loss: 2.24147 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.74916 Validation Accuracy: 0.3292\n",
      "Epoch 12, CIFAR-10 Batch 2:  Batch Loss: 1.84373 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.76186 Validation Accuracy: 0.3174\n",
      "Epoch 12, CIFAR-10 Batch 3:  Batch Loss: 1.54008 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.75166 Validation Accuracy: 0.3298\n",
      "Epoch 12, CIFAR-10 Batch 4:  Batch Loss: 1.67622 Batch Accuracy: 0.35\n",
      "Validation Loss: 1.74623 Validation Accuracy: 0.3272\n",
      "Epoch 12, CIFAR-10 Batch 5:  Batch Loss: 1.81521 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.73562 Validation Accuracy: 0.3466\n",
      "Epoch 13, CIFAR-10 Batch 1:  Batch Loss: 2.19021 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.72853 Validation Accuracy: 0.3398\n",
      "Epoch 13, CIFAR-10 Batch 2:  Batch Loss: 1.80962 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.73854 Validation Accuracy: 0.331\n",
      "Epoch 13, CIFAR-10 Batch 3:  Batch Loss: 1.50354 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.7297 Validation Accuracy: 0.3436\n",
      "Epoch 13, CIFAR-10 Batch 4:  Batch Loss: 1.65891 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.72343 Validation Accuracy: 0.3414\n",
      "Epoch 13, CIFAR-10 Batch 5:  Batch Loss: 1.79971 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.71936 Validation Accuracy: 0.3566\n",
      "Epoch 14, CIFAR-10 Batch 1:  Batch Loss: 2.12749 Batch Accuracy: 0.275\n",
      "Validation Loss: 1.707 Validation Accuracy: 0.3528\n",
      "Epoch 14, CIFAR-10 Batch 2:  Batch Loss: 1.79989 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.71678 Validation Accuracy: 0.3404\n",
      "Epoch 14, CIFAR-10 Batch 3:  Batch Loss: 1.4566 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.71286 Validation Accuracy: 0.3516\n",
      "Epoch 14, CIFAR-10 Batch 4:  Batch Loss: 1.6396 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.70246 Validation Accuracy: 0.3504\n",
      "Epoch 14, CIFAR-10 Batch 5:  Batch Loss: 1.75883 Batch Accuracy: 0.3\n",
      "Validation Loss: 1.70113 Validation Accuracy: 0.3662\n",
      "Epoch 15, CIFAR-10 Batch 1:  Batch Loss: 2.10153 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.68742 Validation Accuracy: 0.359\n",
      "Epoch 15, CIFAR-10 Batch 2:  Batch Loss: 1.78956 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.69554 Validation Accuracy: 0.3508\n",
      "Epoch 15, CIFAR-10 Batch 3:  Batch Loss: 1.42242 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.69278 Validation Accuracy: 0.3618\n",
      "Epoch 15, CIFAR-10 Batch 4:  Batch Loss: 1.62241 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.68131 Validation Accuracy: 0.3612\n",
      "Epoch 15, CIFAR-10 Batch 5:  Batch Loss: 1.70685 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.67648 Validation Accuracy: 0.3708\n",
      "Epoch 16, CIFAR-10 Batch 1:  Batch Loss: 2.07437 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.6647 Validation Accuracy: 0.3682\n",
      "Epoch 16, CIFAR-10 Batch 2:  Batch Loss: 1.74929 Batch Accuracy: 0.325\n",
      "Validation Loss: 1.67192 Validation Accuracy: 0.3612\n",
      "Epoch 16, CIFAR-10 Batch 3:  Batch Loss: 1.40115 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.67206 Validation Accuracy: 0.3718\n",
      "Epoch 16, CIFAR-10 Batch 4:  Batch Loss: 1.61514 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.65971 Validation Accuracy: 0.3742\n",
      "Epoch 16, CIFAR-10 Batch 5:  Batch Loss: 1.66097 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.65366 Validation Accuracy: 0.3822\n",
      "Epoch 17, CIFAR-10 Batch 1:  Batch Loss: 2.06692 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.64236 Validation Accuracy: 0.3832\n",
      "Epoch 17, CIFAR-10 Batch 2:  Batch Loss: 1.69678 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.64891 Validation Accuracy: 0.3744\n",
      "Epoch 17, CIFAR-10 Batch 3:  Batch Loss: 1.37268 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.65106 Validation Accuracy: 0.38\n",
      "Epoch 17, CIFAR-10 Batch 4:  Batch Loss: 1.60901 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.63951 Validation Accuracy: 0.3832\n",
      "Epoch 17, CIFAR-10 Batch 5:  Batch Loss: 1.62597 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.63344 Validation Accuracy: 0.3894\n",
      "Epoch 18, CIFAR-10 Batch 1:  Batch Loss: 2.05462 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.61967 Validation Accuracy: 0.3928\n",
      "Epoch 18, CIFAR-10 Batch 2:  Batch Loss: 1.67025 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.62998 Validation Accuracy: 0.3846\n",
      "Epoch 18, CIFAR-10 Batch 3:  Batch Loss: 1.3437 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.63345 Validation Accuracy: 0.3912\n",
      "Epoch 18, CIFAR-10 Batch 4:  Batch Loss: 1.58957 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.61889 Validation Accuracy: 0.3932\n",
      "Epoch 18, CIFAR-10 Batch 5:  Batch Loss: 1.60365 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.61676 Validation Accuracy: 0.3982\n",
      "Epoch 19, CIFAR-10 Batch 1:  Batch Loss: 2.02531 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.6019 Validation Accuracy: 0.3998\n",
      "Epoch 19, CIFAR-10 Batch 2:  Batch Loss: 1.62501 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.61166 Validation Accuracy: 0.3982\n",
      "Epoch 19, CIFAR-10 Batch 3:  Batch Loss: 1.32277 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.61863 Validation Accuracy: 0.4012\n",
      "Epoch 19, CIFAR-10 Batch 4:  Batch Loss: 1.57773 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.60366 Validation Accuracy: 0.401\n",
      "Epoch 19, CIFAR-10 Batch 5:  Batch Loss: 1.57146 Batch Accuracy: 0.375\n",
      "Validation Loss: 1.60268 Validation Accuracy: 0.4056\n",
      "Epoch 20, CIFAR-10 Batch 1:  Batch Loss: 1.99908 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.5876 Validation Accuracy: 0.4082\n",
      "Epoch 20, CIFAR-10 Batch 2:  Batch Loss: 1.58523 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.59791 Validation Accuracy: 0.4054\n",
      "Epoch 20, CIFAR-10 Batch 3:  Batch Loss: 1.29058 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.60585 Validation Accuracy: 0.405\n",
      "Epoch 20, CIFAR-10 Batch 4:  Batch Loss: 1.54933 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.5867 Validation Accuracy: 0.4074\n",
      "Epoch 20, CIFAR-10 Batch 5:  Batch Loss: 1.54056 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.58544 Validation Accuracy: 0.4124\n",
      "Epoch 21, CIFAR-10 Batch 1:  Batch Loss: 1.96273 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.57445 Validation Accuracy: 0.413\n",
      "Epoch 21, CIFAR-10 Batch 2:  Batch Loss: 1.54973 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.5865 Validation Accuracy: 0.4092\n",
      "Epoch 21, CIFAR-10 Batch 3:  Batch Loss: 1.24958 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.59466 Validation Accuracy: 0.4072\n",
      "Epoch 21, CIFAR-10 Batch 4:  Batch Loss: 1.52783 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.57475 Validation Accuracy: 0.411\n",
      "Epoch 21, CIFAR-10 Batch 5:  Batch Loss: 1.51772 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.57183 Validation Accuracy: 0.4186\n",
      "Epoch 22, CIFAR-10 Batch 1:  Batch Loss: 1.92637 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.5602 Validation Accuracy: 0.4134\n",
      "Epoch 22, CIFAR-10 Batch 2:  Batch Loss: 1.52448 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.57364 Validation Accuracy: 0.4196\n",
      "Epoch 22, CIFAR-10 Batch 3:  Batch Loss: 1.21548 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.5812 Validation Accuracy: 0.4128\n",
      "Epoch 22, CIFAR-10 Batch 4:  Batch Loss: 1.49552 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.5627 Validation Accuracy: 0.414\n",
      "Epoch 22, CIFAR-10 Batch 5:  Batch Loss: 1.49501 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.55952 Validation Accuracy: 0.4212\n",
      "Epoch 23, CIFAR-10 Batch 1:  Batch Loss: 1.89642 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.54932 Validation Accuracy: 0.422\n",
      "Epoch 23, CIFAR-10 Batch 2:  Batch Loss: 1.49813 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.56122 Validation Accuracy: 0.4232\n",
      "Epoch 23, CIFAR-10 Batch 3:  Batch Loss: 1.18121 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.56951 Validation Accuracy: 0.4192\n",
      "Epoch 23, CIFAR-10 Batch 4:  Batch Loss: 1.46778 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.55158 Validation Accuracy: 0.4178\n",
      "Epoch 23, CIFAR-10 Batch 5:  Batch Loss: 1.46662 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.54934 Validation Accuracy: 0.427\n",
      "Epoch 24, CIFAR-10 Batch 1:  Batch Loss: 1.85231 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.53904 Validation Accuracy: 0.427\n",
      "Epoch 24, CIFAR-10 Batch 2:  Batch Loss: 1.47059 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.55319 Validation Accuracy: 0.427\n",
      "Epoch 24, CIFAR-10 Batch 3:  Batch Loss: 1.14755 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.56147 Validation Accuracy: 0.4192\n",
      "Epoch 24, CIFAR-10 Batch 4:  Batch Loss: 1.44111 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.54392 Validation Accuracy: 0.4212\n",
      "Epoch 24, CIFAR-10 Batch 5:  Batch Loss: 1.44108 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.53894 Validation Accuracy: 0.4268\n",
      "Epoch 25, CIFAR-10 Batch 1:  Batch Loss: 1.81613 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.53246 Validation Accuracy: 0.4302\n",
      "Epoch 25, CIFAR-10 Batch 2:  Batch Loss: 1.4374 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.54017 Validation Accuracy: 0.429\n",
      "Epoch 25, CIFAR-10 Batch 3:  Batch Loss: 1.12301 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.55401 Validation Accuracy: 0.4206\n",
      "Epoch 25, CIFAR-10 Batch 4:  Batch Loss: 1.4391 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.53779 Validation Accuracy: 0.4256\n",
      "Epoch 25, CIFAR-10 Batch 5:  Batch Loss: 1.4322 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.53144 Validation Accuracy: 0.4318\n",
      "Epoch 26, CIFAR-10 Batch 1:  Batch Loss: 1.7895 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.52266 Validation Accuracy: 0.4332\n",
      "Epoch 26, CIFAR-10 Batch 2:  Batch Loss: 1.41448 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.53644 Validation Accuracy: 0.4296\n",
      "Epoch 26, CIFAR-10 Batch 3:  Batch Loss: 1.10274 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.54546 Validation Accuracy: 0.4266\n",
      "Epoch 26, CIFAR-10 Batch 4:  Batch Loss: 1.41173 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.53093 Validation Accuracy: 0.4272\n",
      "Epoch 26, CIFAR-10 Batch 5:  Batch Loss: 1.4082 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.52274 Validation Accuracy: 0.4352\n",
      "Epoch 27, CIFAR-10 Batch 1:  Batch Loss: 1.75202 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.51497 Validation Accuracy: 0.437\n",
      "Epoch 27, CIFAR-10 Batch 2:  Batch Loss: 1.40041 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.53237 Validation Accuracy: 0.4298\n",
      "Epoch 27, CIFAR-10 Batch 3:  Batch Loss: 1.08557 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.53884 Validation Accuracy: 0.429\n",
      "Epoch 27, CIFAR-10 Batch 4:  Batch Loss: 1.40283 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.5247 Validation Accuracy: 0.4298\n",
      "Epoch 27, CIFAR-10 Batch 5:  Batch Loss: 1.38697 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.5176 Validation Accuracy: 0.4348\n",
      "Epoch 28, CIFAR-10 Batch 1:  Batch Loss: 1.72915 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.50928 Validation Accuracy: 0.4408\n",
      "Epoch 28, CIFAR-10 Batch 2:  Batch Loss: 1.3741 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.52512 Validation Accuracy: 0.4366\n",
      "Epoch 28, CIFAR-10 Batch 3:  Batch Loss: 1.07402 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.53267 Validation Accuracy: 0.4348\n",
      "Epoch 28, CIFAR-10 Batch 4:  Batch Loss: 1.38952 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.51545 Validation Accuracy: 0.438\n",
      "Epoch 28, CIFAR-10 Batch 5:  Batch Loss: 1.36435 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.50914 Validation Accuracy: 0.4392\n",
      "Epoch 29, CIFAR-10 Batch 1:  Batch Loss: 1.70585 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.50122 Validation Accuracy: 0.4416\n",
      "Epoch 29, CIFAR-10 Batch 2:  Batch Loss: 1.35906 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.51791 Validation Accuracy: 0.4376\n",
      "Epoch 29, CIFAR-10 Batch 3:  Batch Loss: 1.06586 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.52666 Validation Accuracy: 0.4408\n",
      "Epoch 29, CIFAR-10 Batch 4:  Batch Loss: 1.38174 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.51049 Validation Accuracy: 0.4372\n",
      "Epoch 29, CIFAR-10 Batch 5:  Batch Loss: 1.34575 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.5034 Validation Accuracy: 0.4388\n",
      "Epoch 30, CIFAR-10 Batch 1:  Batch Loss: 1.67533 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.49613 Validation Accuracy: 0.4406\n",
      "Epoch 30, CIFAR-10 Batch 2:  Batch Loss: 1.34853 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.51086 Validation Accuracy: 0.4394\n",
      "Epoch 30, CIFAR-10 Batch 3:  Batch Loss: 1.04821 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.52214 Validation Accuracy: 0.4436\n",
      "Epoch 30, CIFAR-10 Batch 4:  Batch Loss: 1.37101 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.50366 Validation Accuracy: 0.4398\n",
      "Epoch 30, CIFAR-10 Batch 5:  Batch Loss: 1.32842 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.49858 Validation Accuracy: 0.4394\n",
      "Epoch 31, CIFAR-10 Batch 1:  Batch Loss: 1.65289 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.49009 Validation Accuracy: 0.444\n",
      "Epoch 31, CIFAR-10 Batch 2:  Batch Loss: 1.33943 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.50497 Validation Accuracy: 0.4404\n",
      "Epoch 31, CIFAR-10 Batch 3:  Batch Loss: 1.03885 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.51616 Validation Accuracy: 0.4426\n",
      "Epoch 31, CIFAR-10 Batch 4:  Batch Loss: 1.36186 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.50107 Validation Accuracy: 0.4412\n",
      "Epoch 31, CIFAR-10 Batch 5:  Batch Loss: 1.31143 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.49456 Validation Accuracy: 0.4414\n",
      "Epoch 32, CIFAR-10 Batch 1:  Batch Loss: 1.6341 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.48611 Validation Accuracy: 0.4472\n",
      "Epoch 32, CIFAR-10 Batch 2:  Batch Loss: 1.3239 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.49819 Validation Accuracy: 0.4444\n",
      "Epoch 32, CIFAR-10 Batch 3:  Batch Loss: 1.02639 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.51163 Validation Accuracy: 0.4492\n",
      "Epoch 32, CIFAR-10 Batch 4:  Batch Loss: 1.3518 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.49534 Validation Accuracy: 0.444\n",
      "Epoch 32, CIFAR-10 Batch 5:  Batch Loss: 1.29643 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.48783 Validation Accuracy: 0.441\n",
      "Epoch 33, CIFAR-10 Batch 1:  Batch Loss: 1.61409 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.48184 Validation Accuracy: 0.4492\n",
      "Epoch 33, CIFAR-10 Batch 2:  Batch Loss: 1.31635 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.49458 Validation Accuracy: 0.4452\n",
      "Epoch 33, CIFAR-10 Batch 3:  Batch Loss: 1.0198 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.50785 Validation Accuracy: 0.4482\n",
      "Epoch 33, CIFAR-10 Batch 4:  Batch Loss: 1.34231 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.48825 Validation Accuracy: 0.4464\n",
      "Epoch 33, CIFAR-10 Batch 5:  Batch Loss: 1.28027 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.48379 Validation Accuracy: 0.443\n",
      "Epoch 34, CIFAR-10 Batch 1:  Batch Loss: 1.6005 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.47722 Validation Accuracy: 0.4528\n",
      "Epoch 34, CIFAR-10 Batch 2:  Batch Loss: 1.31223 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.48924 Validation Accuracy: 0.4452\n",
      "Epoch 34, CIFAR-10 Batch 3:  Batch Loss: 1.00802 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.5044 Validation Accuracy: 0.451\n",
      "Epoch 34, CIFAR-10 Batch 4:  Batch Loss: 1.32944 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.48511 Validation Accuracy: 0.4488\n",
      "Epoch 34, CIFAR-10 Batch 5:  Batch Loss: 1.26124 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.47717 Validation Accuracy: 0.4478\n",
      "Epoch 35, CIFAR-10 Batch 1:  Batch Loss: 1.58237 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.47768 Validation Accuracy: 0.451\n",
      "Epoch 35, CIFAR-10 Batch 2:  Batch Loss: 1.30319 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.48383 Validation Accuracy: 0.4486\n",
      "Epoch 35, CIFAR-10 Batch 3:  Batch Loss: 1.00001 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.50037 Validation Accuracy: 0.4546\n",
      "Epoch 35, CIFAR-10 Batch 4:  Batch Loss: 1.32538 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.48058 Validation Accuracy: 0.449\n",
      "Epoch 35, CIFAR-10 Batch 5:  Batch Loss: 1.25103 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.47206 Validation Accuracy: 0.4474\n",
      "Epoch 36, CIFAR-10 Batch 1:  Batch Loss: 1.56999 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.47171 Validation Accuracy: 0.4534\n",
      "Epoch 36, CIFAR-10 Batch 2:  Batch Loss: 1.29657 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.48026 Validation Accuracy: 0.4494\n",
      "Epoch 36, CIFAR-10 Batch 3:  Batch Loss: 0.994373 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.4984 Validation Accuracy: 0.454\n",
      "Epoch 36, CIFAR-10 Batch 4:  Batch Loss: 1.32269 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.47678 Validation Accuracy: 0.447\n",
      "Epoch 36, CIFAR-10 Batch 5:  Batch Loss: 1.23363 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.46877 Validation Accuracy: 0.4472\n",
      "Epoch 37, CIFAR-10 Batch 1:  Batch Loss: 1.54988 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.46564 Validation Accuracy: 0.4554\n",
      "Epoch 37, CIFAR-10 Batch 2:  Batch Loss: 1.29013 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.47648 Validation Accuracy: 0.451\n",
      "Epoch 37, CIFAR-10 Batch 3:  Batch Loss: 0.990153 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.49517 Validation Accuracy: 0.4566\n",
      "Epoch 37, CIFAR-10 Batch 4:  Batch Loss: 1.31695 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.47324 Validation Accuracy: 0.448\n",
      "Epoch 37, CIFAR-10 Batch 5:  Batch Loss: 1.21935 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.46658 Validation Accuracy: 0.4496\n",
      "Epoch 38, CIFAR-10 Batch 1:  Batch Loss: 1.54423 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.46326 Validation Accuracy: 0.4544\n",
      "Epoch 38, CIFAR-10 Batch 2:  Batch Loss: 1.28063 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.47157 Validation Accuracy: 0.4538\n",
      "Epoch 38, CIFAR-10 Batch 3:  Batch Loss: 0.979555 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.49225 Validation Accuracy: 0.4582\n",
      "Epoch 38, CIFAR-10 Batch 4:  Batch Loss: 1.30567 Batch Accuracy: 0.4\n",
      "Validation Loss: 1.46999 Validation Accuracy: 0.4484\n",
      "Epoch 38, CIFAR-10 Batch 5:  Batch Loss: 1.2059 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.46163 Validation Accuracy: 0.4518\n",
      "Epoch 39, CIFAR-10 Batch 1:  Batch Loss: 1.51809 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.45957 Validation Accuracy: 0.4566\n",
      "Epoch 39, CIFAR-10 Batch 2:  Batch Loss: 1.26982 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.46907 Validation Accuracy: 0.4568\n",
      "Epoch 39, CIFAR-10 Batch 3:  Batch Loss: 0.967716 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.48879 Validation Accuracy: 0.4606\n",
      "Epoch 39, CIFAR-10 Batch 4:  Batch Loss: 1.299 Batch Accuracy: 0.425\n",
      "Validation Loss: 1.4676 Validation Accuracy: 0.4506\n",
      "Epoch 39, CIFAR-10 Batch 5:  Batch Loss: 1.19378 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.45833 Validation Accuracy: 0.4544\n",
      "Epoch 40, CIFAR-10 Batch 1:  Batch Loss: 1.50073 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.45578 Validation Accuracy: 0.459\n",
      "Epoch 40, CIFAR-10 Batch 2:  Batch Loss: 1.26631 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.46493 Validation Accuracy: 0.4576\n",
      "Epoch 40, CIFAR-10 Batch 3:  Batch Loss: 0.961872 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.48455 Validation Accuracy: 0.4618\n",
      "Epoch 40, CIFAR-10 Batch 4:  Batch Loss: 1.28322 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.46517 Validation Accuracy: 0.4508\n",
      "Epoch 40, CIFAR-10 Batch 5:  Batch Loss: 1.18534 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.45471 Validation Accuracy: 0.4578\n",
      "Epoch 41, CIFAR-10 Batch 1:  Batch Loss: 1.49702 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.45201 Validation Accuracy: 0.4606\n",
      "Epoch 41, CIFAR-10 Batch 2:  Batch Loss: 1.25429 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.46104 Validation Accuracy: 0.4622\n",
      "Epoch 41, CIFAR-10 Batch 3:  Batch Loss: 0.952042 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.48174 Validation Accuracy: 0.4588\n",
      "Epoch 41, CIFAR-10 Batch 4:  Batch Loss: 1.27362 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.46122 Validation Accuracy: 0.451\n",
      "Epoch 41, CIFAR-10 Batch 5:  Batch Loss: 1.1742 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.45123 Validation Accuracy: 0.4596\n",
      "Epoch 42, CIFAR-10 Batch 1:  Batch Loss: 1.48399 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.44719 Validation Accuracy: 0.464\n",
      "Epoch 42, CIFAR-10 Batch 2:  Batch Loss: 1.24725 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.45924 Validation Accuracy: 0.464\n",
      "Epoch 42, CIFAR-10 Batch 3:  Batch Loss: 0.947651 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.47874 Validation Accuracy: 0.461\n",
      "Epoch 42, CIFAR-10 Batch 4:  Batch Loss: 1.26873 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.46096 Validation Accuracy: 0.4528\n",
      "Epoch 42, CIFAR-10 Batch 5:  Batch Loss: 1.16164 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.44734 Validation Accuracy: 0.4642\n",
      "Epoch 43, CIFAR-10 Batch 1:  Batch Loss: 1.47328 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.4426 Validation Accuracy: 0.4674\n",
      "Epoch 43, CIFAR-10 Batch 2:  Batch Loss: 1.23739 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.45601 Validation Accuracy: 0.467\n",
      "Epoch 43, CIFAR-10 Batch 3:  Batch Loss: 0.9413 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.47674 Validation Accuracy: 0.4598\n",
      "Epoch 43, CIFAR-10 Batch 4:  Batch Loss: 1.25708 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.45796 Validation Accuracy: 0.4542\n",
      "Epoch 43, CIFAR-10 Batch 5:  Batch Loss: 1.15354 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.44335 Validation Accuracy: 0.4656\n",
      "Epoch 44, CIFAR-10 Batch 1:  Batch Loss: 1.46144 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.4396 Validation Accuracy: 0.4678\n",
      "Epoch 44, CIFAR-10 Batch 2:  Batch Loss: 1.233 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.45328 Validation Accuracy: 0.4684\n",
      "Epoch 44, CIFAR-10 Batch 3:  Batch Loss: 0.935914 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.47411 Validation Accuracy: 0.4604\n",
      "Epoch 44, CIFAR-10 Batch 4:  Batch Loss: 1.24747 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.45549 Validation Accuracy: 0.457\n",
      "Epoch 44, CIFAR-10 Batch 5:  Batch Loss: 1.14685 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.44095 Validation Accuracy: 0.467\n",
      "Epoch 45, CIFAR-10 Batch 1:  Batch Loss: 1.45686 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.43609 Validation Accuracy: 0.4732\n",
      "Epoch 45, CIFAR-10 Batch 2:  Batch Loss: 1.21796 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.44977 Validation Accuracy: 0.4724\n",
      "Epoch 45, CIFAR-10 Batch 3:  Batch Loss: 0.930414 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.47197 Validation Accuracy: 0.4608\n",
      "Epoch 45, CIFAR-10 Batch 4:  Batch Loss: 1.24694 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.45285 Validation Accuracy: 0.4564\n",
      "Epoch 45, CIFAR-10 Batch 5:  Batch Loss: 1.13722 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.4399 Validation Accuracy: 0.4682\n",
      "Epoch 46, CIFAR-10 Batch 1:  Batch Loss: 1.45065 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.43236 Validation Accuracy: 0.474\n",
      "Epoch 46, CIFAR-10 Batch 2:  Batch Loss: 1.21077 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.44652 Validation Accuracy: 0.4754\n",
      "Epoch 46, CIFAR-10 Batch 3:  Batch Loss: 0.922584 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.47086 Validation Accuracy: 0.4624\n",
      "Epoch 46, CIFAR-10 Batch 4:  Batch Loss: 1.2343 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.45232 Validation Accuracy: 0.4584\n",
      "Epoch 46, CIFAR-10 Batch 5:  Batch Loss: 1.13429 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.43659 Validation Accuracy: 0.4694\n",
      "Epoch 47, CIFAR-10 Batch 1:  Batch Loss: 1.44908 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.43092 Validation Accuracy: 0.4732\n",
      "Epoch 47, CIFAR-10 Batch 2:  Batch Loss: 1.2022 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.44646 Validation Accuracy: 0.4738\n",
      "Epoch 47, CIFAR-10 Batch 3:  Batch Loss: 0.918727 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.46846 Validation Accuracy: 0.4662\n",
      "Epoch 47, CIFAR-10 Batch 4:  Batch Loss: 1.22521 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.44949 Validation Accuracy: 0.4606\n",
      "Epoch 47, CIFAR-10 Batch 5:  Batch Loss: 1.12425 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.43391 Validation Accuracy: 0.474\n",
      "Epoch 48, CIFAR-10 Batch 1:  Batch Loss: 1.44055 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.42618 Validation Accuracy: 0.4764\n",
      "Epoch 48, CIFAR-10 Batch 2:  Batch Loss: 1.19542 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.44353 Validation Accuracy: 0.4764\n",
      "Epoch 48, CIFAR-10 Batch 3:  Batch Loss: 0.909908 Batch Accuracy: 0.7\n",
      "Validation Loss: 1.46647 Validation Accuracy: 0.4662\n",
      "Epoch 48, CIFAR-10 Batch 4:  Batch Loss: 1.20972 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.44557 Validation Accuracy: 0.4626\n",
      "Epoch 48, CIFAR-10 Batch 5:  Batch Loss: 1.11942 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.43435 Validation Accuracy: 0.4742\n",
      "Epoch 49, CIFAR-10 Batch 1:  Batch Loss: 1.42747 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.42592 Validation Accuracy: 0.4744\n",
      "Epoch 49, CIFAR-10 Batch 2:  Batch Loss: 1.18735 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.44056 Validation Accuracy: 0.477\n",
      "Epoch 49, CIFAR-10 Batch 3:  Batch Loss: 0.904512 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.46329 Validation Accuracy: 0.4674\n",
      "Epoch 49, CIFAR-10 Batch 4:  Batch Loss: 1.20449 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.44342 Validation Accuracy: 0.4636\n",
      "Epoch 49, CIFAR-10 Batch 5:  Batch Loss: 1.11492 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.4323 Validation Accuracy: 0.4736\n",
      "Epoch 50, CIFAR-10 Batch 1:  Batch Loss: 1.4231 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.4229 Validation Accuracy: 0.4772\n",
      "Epoch 50, CIFAR-10 Batch 2:  Batch Loss: 1.17823 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.43644 Validation Accuracy: 0.4782\n",
      "Epoch 50, CIFAR-10 Batch 3:  Batch Loss: 0.893013 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.46224 Validation Accuracy: 0.4678\n",
      "Epoch 50, CIFAR-10 Batch 4:  Batch Loss: 1.19423 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.44229 Validation Accuracy: 0.4636\n",
      "Epoch 50, CIFAR-10 Batch 5:  Batch Loss: 1.10415 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.43052 Validation Accuracy: 0.4758\n",
      "Epoch 51, CIFAR-10 Batch 1:  Batch Loss: 1.41698 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.42072 Validation Accuracy: 0.4772\n",
      "Epoch 51, CIFAR-10 Batch 2:  Batch Loss: 1.16875 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.43612 Validation Accuracy: 0.4792\n",
      "Epoch 51, CIFAR-10 Batch 3:  Batch Loss: 0.885202 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.46123 Validation Accuracy: 0.4704\n",
      "Epoch 51, CIFAR-10 Batch 4:  Batch Loss: 1.18487 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.44126 Validation Accuracy: 0.4666\n",
      "Epoch 51, CIFAR-10 Batch 5:  Batch Loss: 1.09539 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.43084 Validation Accuracy: 0.477\n",
      "Epoch 52, CIFAR-10 Batch 1:  Batch Loss: 1.40839 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.41885 Validation Accuracy: 0.4802\n",
      "Epoch 52, CIFAR-10 Batch 2:  Batch Loss: 1.16648 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.43584 Validation Accuracy: 0.4788\n",
      "Epoch 52, CIFAR-10 Batch 3:  Batch Loss: 0.880891 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.45958 Validation Accuracy: 0.4706\n",
      "Epoch 52, CIFAR-10 Batch 4:  Batch Loss: 1.18035 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.43858 Validation Accuracy: 0.4686\n",
      "Epoch 52, CIFAR-10 Batch 5:  Batch Loss: 1.08823 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.42913 Validation Accuracy: 0.479\n",
      "Epoch 53, CIFAR-10 Batch 1:  Batch Loss: 1.40923 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.41689 Validation Accuracy: 0.4832\n",
      "Epoch 53, CIFAR-10 Batch 2:  Batch Loss: 1.16132 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.4318 Validation Accuracy: 0.4798\n",
      "Epoch 53, CIFAR-10 Batch 3:  Batch Loss: 0.877086 Batch Accuracy: 0.7\n",
      "Validation Loss: 1.46024 Validation Accuracy: 0.474\n",
      "Epoch 53, CIFAR-10 Batch 4:  Batch Loss: 1.16973 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.4356 Validation Accuracy: 0.4718\n",
      "Epoch 53, CIFAR-10 Batch 5:  Batch Loss: 1.08053 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.42905 Validation Accuracy: 0.4806\n",
      "Epoch 54, CIFAR-10 Batch 1:  Batch Loss: 1.40336 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.41531 Validation Accuracy: 0.483\n",
      "Epoch 54, CIFAR-10 Batch 2:  Batch Loss: 1.1578 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.42876 Validation Accuracy: 0.4818\n",
      "Epoch 54, CIFAR-10 Batch 3:  Batch Loss: 0.871315 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.45748 Validation Accuracy: 0.4758\n",
      "Epoch 54, CIFAR-10 Batch 4:  Batch Loss: 1.16503 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.43127 Validation Accuracy: 0.4732\n",
      "Epoch 54, CIFAR-10 Batch 5:  Batch Loss: 1.0736 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.42598 Validation Accuracy: 0.4818\n",
      "Epoch 55, CIFAR-10 Batch 1:  Batch Loss: 1.40129 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.41444 Validation Accuracy: 0.4878\n",
      "Epoch 55, CIFAR-10 Batch 2:  Batch Loss: 1.15159 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.42658 Validation Accuracy: 0.4828\n",
      "Epoch 55, CIFAR-10 Batch 3:  Batch Loss: 0.864863 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.45479 Validation Accuracy: 0.475\n",
      "Epoch 55, CIFAR-10 Batch 4:  Batch Loss: 1.1572 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.4296 Validation Accuracy: 0.4722\n",
      "Epoch 55, CIFAR-10 Batch 5:  Batch Loss: 1.07259 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.42526 Validation Accuracy: 0.4824\n",
      "Epoch 56, CIFAR-10 Batch 1:  Batch Loss: 1.38944 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.4123 Validation Accuracy: 0.4884\n",
      "Epoch 56, CIFAR-10 Batch 2:  Batch Loss: 1.14464 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.42401 Validation Accuracy: 0.484\n",
      "Epoch 56, CIFAR-10 Batch 3:  Batch Loss: 0.858521 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.45291 Validation Accuracy: 0.4762\n",
      "Epoch 56, CIFAR-10 Batch 4:  Batch Loss: 1.15167 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.42841 Validation Accuracy: 0.4732\n",
      "Epoch 56, CIFAR-10 Batch 5:  Batch Loss: 1.06628 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.42374 Validation Accuracy: 0.4842\n",
      "Epoch 57, CIFAR-10 Batch 1:  Batch Loss: 1.38655 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.41147 Validation Accuracy: 0.4878\n",
      "Epoch 57, CIFAR-10 Batch 2:  Batch Loss: 1.13765 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.4227 Validation Accuracy: 0.4844\n",
      "Epoch 57, CIFAR-10 Batch 3:  Batch Loss: 0.852218 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44793 Validation Accuracy: 0.4788\n",
      "Epoch 57, CIFAR-10 Batch 4:  Batch Loss: 1.14547 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.42635 Validation Accuracy: 0.474\n",
      "Epoch 57, CIFAR-10 Batch 5:  Batch Loss: 1.05989 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.42189 Validation Accuracy: 0.4824\n",
      "Epoch 58, CIFAR-10 Batch 1:  Batch Loss: 1.37865 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.4092 Validation Accuracy: 0.49\n",
      "Epoch 58, CIFAR-10 Batch 2:  Batch Loss: 1.12813 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.42068 Validation Accuracy: 0.4864\n",
      "Epoch 58, CIFAR-10 Batch 3:  Batch Loss: 0.846813 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44854 Validation Accuracy: 0.4788\n",
      "Epoch 58, CIFAR-10 Batch 4:  Batch Loss: 1.13578 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.42603 Validation Accuracy: 0.4742\n",
      "Epoch 58, CIFAR-10 Batch 5:  Batch Loss: 1.05516 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41994 Validation Accuracy: 0.4834\n",
      "Epoch 59, CIFAR-10 Batch 1:  Batch Loss: 1.36846 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.40953 Validation Accuracy: 0.49\n",
      "Epoch 59, CIFAR-10 Batch 2:  Batch Loss: 1.12345 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41967 Validation Accuracy: 0.4862\n",
      "Epoch 59, CIFAR-10 Batch 3:  Batch Loss: 0.836588 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44723 Validation Accuracy: 0.479\n",
      "Epoch 59, CIFAR-10 Batch 4:  Batch Loss: 1.13309 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.42223 Validation Accuracy: 0.4756\n",
      "Epoch 59, CIFAR-10 Batch 5:  Batch Loss: 1.05533 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.41577 Validation Accuracy: 0.4854\n",
      "Epoch 60, CIFAR-10 Batch 1:  Batch Loss: 1.37212 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.40881 Validation Accuracy: 0.4892\n",
      "Epoch 60, CIFAR-10 Batch 2:  Batch Loss: 1.12031 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41718 Validation Accuracy: 0.4864\n",
      "Epoch 60, CIFAR-10 Batch 3:  Batch Loss: 0.828763 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44482 Validation Accuracy: 0.4794\n",
      "Epoch 60, CIFAR-10 Batch 4:  Batch Loss: 1.13025 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.42119 Validation Accuracy: 0.476\n",
      "Epoch 60, CIFAR-10 Batch 5:  Batch Loss: 1.0428 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41487 Validation Accuracy: 0.4866\n",
      "Epoch 61, CIFAR-10 Batch 1:  Batch Loss: 1.35647 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.40496 Validation Accuracy: 0.491\n",
      "Epoch 61, CIFAR-10 Batch 2:  Batch Loss: 1.11262 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41576 Validation Accuracy: 0.4882\n",
      "Epoch 61, CIFAR-10 Batch 3:  Batch Loss: 0.822296 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44415 Validation Accuracy: 0.4792\n",
      "Epoch 61, CIFAR-10 Batch 4:  Batch Loss: 1.12365 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.42025 Validation Accuracy: 0.4756\n",
      "Epoch 61, CIFAR-10 Batch 5:  Batch Loss: 1.04393 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41429 Validation Accuracy: 0.4848\n",
      "Epoch 62, CIFAR-10 Batch 1:  Batch Loss: 1.35421 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.40356 Validation Accuracy: 0.488\n",
      "Epoch 62, CIFAR-10 Batch 2:  Batch Loss: 1.11119 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.4131 Validation Accuracy: 0.4884\n",
      "Epoch 62, CIFAR-10 Batch 3:  Batch Loss: 0.816149 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44265 Validation Accuracy: 0.4806\n",
      "Epoch 62, CIFAR-10 Batch 4:  Batch Loss: 1.11996 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.41807 Validation Accuracy: 0.4776\n",
      "Epoch 62, CIFAR-10 Batch 5:  Batch Loss: 1.04201 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.41174 Validation Accuracy: 0.4884\n",
      "Epoch 63, CIFAR-10 Batch 1:  Batch Loss: 1.34335 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.40294 Validation Accuracy: 0.4896\n",
      "Epoch 63, CIFAR-10 Batch 2:  Batch Loss: 1.10263 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41145 Validation Accuracy: 0.4902\n",
      "Epoch 63, CIFAR-10 Batch 3:  Batch Loss: 0.808333 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.4404 Validation Accuracy: 0.4842\n",
      "Epoch 63, CIFAR-10 Batch 4:  Batch Loss: 1.11461 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.41613 Validation Accuracy: 0.4776\n",
      "Epoch 63, CIFAR-10 Batch 5:  Batch Loss: 1.03723 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.41112 Validation Accuracy: 0.4858\n",
      "Epoch 64, CIFAR-10 Batch 1:  Batch Loss: 1.34586 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.40119 Validation Accuracy: 0.4884\n",
      "Epoch 64, CIFAR-10 Batch 2:  Batch Loss: 1.09093 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.41048 Validation Accuracy: 0.489\n",
      "Epoch 64, CIFAR-10 Batch 3:  Batch Loss: 0.803638 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.44044 Validation Accuracy: 0.4822\n",
      "Epoch 64, CIFAR-10 Batch 4:  Batch Loss: 1.11411 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.41354 Validation Accuracy: 0.4774\n",
      "Epoch 64, CIFAR-10 Batch 5:  Batch Loss: 1.02959 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.40837 Validation Accuracy: 0.487\n",
      "Epoch 65, CIFAR-10 Batch 1:  Batch Loss: 1.34309 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.40172 Validation Accuracy: 0.4916\n",
      "Epoch 65, CIFAR-10 Batch 2:  Batch Loss: 1.08585 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40803 Validation Accuracy: 0.4906\n",
      "Epoch 65, CIFAR-10 Batch 3:  Batch Loss: 0.795612 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43777 Validation Accuracy: 0.4826\n",
      "Epoch 65, CIFAR-10 Batch 4:  Batch Loss: 1.10399 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.4127 Validation Accuracy: 0.4784\n",
      "Epoch 65, CIFAR-10 Batch 5:  Batch Loss: 1.027 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40602 Validation Accuracy: 0.4912\n",
      "Epoch 66, CIFAR-10 Batch 1:  Batch Loss: 1.33532 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.401 Validation Accuracy: 0.4906\n",
      "Epoch 66, CIFAR-10 Batch 2:  Batch Loss: 1.07878 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.40693 Validation Accuracy: 0.4914\n",
      "Epoch 66, CIFAR-10 Batch 3:  Batch Loss: 0.790479 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43724 Validation Accuracy: 0.484\n",
      "Epoch 66, CIFAR-10 Batch 4:  Batch Loss: 1.10328 Batch Accuracy: 0.525\n",
      "Validation Loss: 1.41045 Validation Accuracy: 0.4786\n",
      "Epoch 66, CIFAR-10 Batch 5:  Batch Loss: 1.02291 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40315 Validation Accuracy: 0.4918\n",
      "Epoch 67, CIFAR-10 Batch 1:  Batch Loss: 1.32789 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39943 Validation Accuracy: 0.491\n",
      "Epoch 67, CIFAR-10 Batch 2:  Batch Loss: 1.0794 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40445 Validation Accuracy: 0.4918\n",
      "Epoch 67, CIFAR-10 Batch 3:  Batch Loss: 0.785975 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43653 Validation Accuracy: 0.484\n",
      "Epoch 67, CIFAR-10 Batch 4:  Batch Loss: 1.09508 Batch Accuracy: 0.55\n",
      "Validation Loss: 1.40942 Validation Accuracy: 0.4816\n",
      "Epoch 67, CIFAR-10 Batch 5:  Batch Loss: 1.01858 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40409 Validation Accuracy: 0.4898\n",
      "Epoch 68, CIFAR-10 Batch 1:  Batch Loss: 1.32673 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39926 Validation Accuracy: 0.4932\n",
      "Epoch 68, CIFAR-10 Batch 2:  Batch Loss: 1.06873 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40291 Validation Accuracy: 0.4938\n",
      "Epoch 68, CIFAR-10 Batch 3:  Batch Loss: 0.77415 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43633 Validation Accuracy: 0.4842\n",
      "Epoch 68, CIFAR-10 Batch 4:  Batch Loss: 1.08832 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.40654 Validation Accuracy: 0.4842\n",
      "Epoch 68, CIFAR-10 Batch 5:  Batch Loss: 1.01586 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40046 Validation Accuracy: 0.491\n",
      "Epoch 69, CIFAR-10 Batch 1:  Batch Loss: 1.31024 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39675 Validation Accuracy: 0.4902\n",
      "Epoch 69, CIFAR-10 Batch 2:  Batch Loss: 1.06523 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.39984 Validation Accuracy: 0.495\n",
      "Epoch 69, CIFAR-10 Batch 3:  Batch Loss: 0.769523 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43179 Validation Accuracy: 0.4878\n",
      "Epoch 69, CIFAR-10 Batch 4:  Batch Loss: 1.08471 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.40615 Validation Accuracy: 0.4836\n",
      "Epoch 69, CIFAR-10 Batch 5:  Batch Loss: 1.01165 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39942 Validation Accuracy: 0.4932\n",
      "Epoch 70, CIFAR-10 Batch 1:  Batch Loss: 1.30732 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39666 Validation Accuracy: 0.4922\n",
      "Epoch 70, CIFAR-10 Batch 2:  Batch Loss: 1.05687 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40185 Validation Accuracy: 0.4932\n",
      "Epoch 70, CIFAR-10 Batch 3:  Batch Loss: 0.765714 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43098 Validation Accuracy: 0.4864\n",
      "Epoch 70, CIFAR-10 Batch 4:  Batch Loss: 1.08203 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.40375 Validation Accuracy: 0.486\n",
      "Epoch 70, CIFAR-10 Batch 5:  Batch Loss: 1.00826 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39816 Validation Accuracy: 0.4926\n",
      "Epoch 71, CIFAR-10 Batch 1:  Batch Loss: 1.30984 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39667 Validation Accuracy: 0.4908\n",
      "Epoch 71, CIFAR-10 Batch 2:  Batch Loss: 1.05141 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.40035 Validation Accuracy: 0.4946\n",
      "Epoch 71, CIFAR-10 Batch 3:  Batch Loss: 0.763722 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43157 Validation Accuracy: 0.4884\n",
      "Epoch 71, CIFAR-10 Batch 4:  Batch Loss: 1.07649 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.3994 Validation Accuracy: 0.4894\n",
      "Epoch 71, CIFAR-10 Batch 5:  Batch Loss: 1.00233 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39662 Validation Accuracy: 0.4928\n",
      "Epoch 72, CIFAR-10 Batch 1:  Batch Loss: 1.30255 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39718 Validation Accuracy: 0.4946\n",
      "Epoch 72, CIFAR-10 Batch 2:  Batch Loss: 1.04851 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39837 Validation Accuracy: 0.4956\n",
      "Epoch 72, CIFAR-10 Batch 3:  Batch Loss: 0.762569 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.43099 Validation Accuracy: 0.4894\n",
      "Epoch 72, CIFAR-10 Batch 4:  Batch Loss: 1.07563 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.4 Validation Accuracy: 0.4866\n",
      "Epoch 72, CIFAR-10 Batch 5:  Batch Loss: 1.00914 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39772 Validation Accuracy: 0.4948\n",
      "Epoch 73, CIFAR-10 Batch 1:  Batch Loss: 1.29756 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39744 Validation Accuracy: 0.495\n",
      "Epoch 73, CIFAR-10 Batch 2:  Batch Loss: 1.04401 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39855 Validation Accuracy: 0.4944\n",
      "Epoch 73, CIFAR-10 Batch 3:  Batch Loss: 0.756544 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.42869 Validation Accuracy: 0.491\n",
      "Epoch 73, CIFAR-10 Batch 4:  Batch Loss: 1.06819 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.3983 Validation Accuracy: 0.4892\n",
      "Epoch 73, CIFAR-10 Batch 5:  Batch Loss: 1.00089 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39533 Validation Accuracy: 0.4936\n",
      "Epoch 74, CIFAR-10 Batch 1:  Batch Loss: 1.29293 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39257 Validation Accuracy: 0.4954\n",
      "Epoch 74, CIFAR-10 Batch 2:  Batch Loss: 1.0398 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39548 Validation Accuracy: 0.4968\n",
      "Epoch 74, CIFAR-10 Batch 3:  Batch Loss: 0.748865 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.42919 Validation Accuracy: 0.4926\n",
      "Epoch 74, CIFAR-10 Batch 4:  Batch Loss: 1.06551 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.39748 Validation Accuracy: 0.4882\n",
      "Epoch 74, CIFAR-10 Batch 5:  Batch Loss: 0.998495 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39602 Validation Accuracy: 0.4942\n",
      "Epoch 75, CIFAR-10 Batch 1:  Batch Loss: 1.28768 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39418 Validation Accuracy: 0.4966\n",
      "Epoch 75, CIFAR-10 Batch 2:  Batch Loss: 1.03637 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.394 Validation Accuracy: 0.494\n",
      "Epoch 75, CIFAR-10 Batch 3:  Batch Loss: 0.746963 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.42774 Validation Accuracy: 0.4906\n",
      "Epoch 75, CIFAR-10 Batch 4:  Batch Loss: 1.06529 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.39429 Validation Accuracy: 0.4902\n",
      "Epoch 75, CIFAR-10 Batch 5:  Batch Loss: 0.984626 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.3932 Validation Accuracy: 0.4968\n",
      "Epoch 76, CIFAR-10 Batch 1:  Batch Loss: 1.29202 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39324 Validation Accuracy: 0.4964\n",
      "Epoch 76, CIFAR-10 Batch 2:  Batch Loss: 1.03291 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39355 Validation Accuracy: 0.4958\n",
      "Epoch 76, CIFAR-10 Batch 3:  Batch Loss: 0.742513 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.42527 Validation Accuracy: 0.4942\n",
      "Epoch 76, CIFAR-10 Batch 4:  Batch Loss: 1.06113 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.3952 Validation Accuracy: 0.4896\n",
      "Epoch 76, CIFAR-10 Batch 5:  Batch Loss: 0.988861 Batch Accuracy: 0.7\n",
      "Validation Loss: 1.39826 Validation Accuracy: 0.493\n",
      "Epoch 77, CIFAR-10 Batch 1:  Batch Loss: 1.28927 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39325 Validation Accuracy: 0.4972\n",
      "Epoch 77, CIFAR-10 Batch 2:  Batch Loss: 1.02618 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.39332 Validation Accuracy: 0.4964\n",
      "Epoch 77, CIFAR-10 Batch 3:  Batch Loss: 0.732451 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.42477 Validation Accuracy: 0.4944\n",
      "Epoch 77, CIFAR-10 Batch 4:  Batch Loss: 1.0576 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.39135 Validation Accuracy: 0.4906\n",
      "Epoch 77, CIFAR-10 Batch 5:  Batch Loss: 0.984293 Batch Accuracy: 0.675\n",
      "Validation Loss: 1.39612 Validation Accuracy: 0.4936\n",
      "Epoch 78, CIFAR-10 Batch 1:  Batch Loss: 1.28515 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39212 Validation Accuracy: 0.4964\n",
      "Epoch 78, CIFAR-10 Batch 2:  Batch Loss: 1.02566 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.39041 Validation Accuracy: 0.496\n",
      "Epoch 78, CIFAR-10 Batch 3:  Batch Loss: 0.726358 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.42429 Validation Accuracy: 0.4938\n",
      "Epoch 78, CIFAR-10 Batch 4:  Batch Loss: 1.05619 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.39165 Validation Accuracy: 0.4902\n",
      "Epoch 78, CIFAR-10 Batch 5:  Batch Loss: 0.975562 Batch Accuracy: 0.7\n",
      "Validation Loss: 1.39673 Validation Accuracy: 0.4972\n",
      "Epoch 79, CIFAR-10 Batch 1:  Batch Loss: 1.28245 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.39172 Validation Accuracy: 0.4982\n",
      "Epoch 79, CIFAR-10 Batch 2:  Batch Loss: 1.02176 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.38876 Validation Accuracy: 0.4962\n",
      "Epoch 79, CIFAR-10 Batch 3:  Batch Loss: 0.72213 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.42368 Validation Accuracy: 0.4934\n",
      "Epoch 79, CIFAR-10 Batch 4:  Batch Loss: 1.05168 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.39136 Validation Accuracy: 0.4916\n",
      "Epoch 79, CIFAR-10 Batch 5:  Batch Loss: 0.97352 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39429 Validation Accuracy: 0.4964\n",
      "Epoch 80, CIFAR-10 Batch 1:  Batch Loss: 1.28328 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38998 Validation Accuracy: 0.4966\n",
      "Epoch 80, CIFAR-10 Batch 2:  Batch Loss: 1.02564 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38724 Validation Accuracy: 0.4976\n",
      "Epoch 80, CIFAR-10 Batch 3:  Batch Loss: 0.718608 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.42089 Validation Accuracy: 0.4936\n",
      "Epoch 80, CIFAR-10 Batch 4:  Batch Loss: 1.05108 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.39091 Validation Accuracy: 0.4916\n",
      "Epoch 80, CIFAR-10 Batch 5:  Batch Loss: 0.961533 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39201 Validation Accuracy: 0.496\n",
      "Epoch 81, CIFAR-10 Batch 1:  Batch Loss: 1.2772 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38944 Validation Accuracy: 0.4956\n",
      "Epoch 81, CIFAR-10 Batch 2:  Batch Loss: 1.01814 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38601 Validation Accuracy: 0.4974\n",
      "Epoch 81, CIFAR-10 Batch 3:  Batch Loss: 0.720667 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.42218 Validation Accuracy: 0.4922\n",
      "Epoch 81, CIFAR-10 Batch 4:  Batch Loss: 1.04572 Batch Accuracy: 0.575\n",
      "Validation Loss: 1.39039 Validation Accuracy: 0.4904\n",
      "Epoch 81, CIFAR-10 Batch 5:  Batch Loss: 0.957587 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39269 Validation Accuracy: 0.4974\n",
      "Epoch 82, CIFAR-10 Batch 1:  Batch Loss: 1.2783 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38601 Validation Accuracy: 0.4972\n",
      "Epoch 82, CIFAR-10 Batch 2:  Batch Loss: 1.01218 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.38108 Validation Accuracy: 0.4994\n",
      "Epoch 82, CIFAR-10 Batch 3:  Batch Loss: 0.709691 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41616 Validation Accuracy: 0.4952\n",
      "Epoch 82, CIFAR-10 Batch 4:  Batch Loss: 1.03808 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38689 Validation Accuracy: 0.4932\n",
      "Epoch 82, CIFAR-10 Batch 5:  Batch Loss: 0.944589 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39087 Validation Accuracy: 0.4948\n",
      "Epoch 83, CIFAR-10 Batch 1:  Batch Loss: 1.2759 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38649 Validation Accuracy: 0.4966\n",
      "Epoch 83, CIFAR-10 Batch 2:  Batch Loss: 1.01521 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38052 Validation Accuracy: 0.502\n",
      "Epoch 83, CIFAR-10 Batch 3:  Batch Loss: 0.708904 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41668 Validation Accuracy: 0.4948\n",
      "Epoch 83, CIFAR-10 Batch 4:  Batch Loss: 1.03611 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.3893 Validation Accuracy: 0.4928\n",
      "Epoch 83, CIFAR-10 Batch 5:  Batch Loss: 0.94619 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39062 Validation Accuracy: 0.4984\n",
      "Epoch 84, CIFAR-10 Batch 1:  Batch Loss: 1.27077 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38521 Validation Accuracy: 0.4996\n",
      "Epoch 84, CIFAR-10 Batch 2:  Batch Loss: 1.00772 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.38188 Validation Accuracy: 0.5\n",
      "Epoch 84, CIFAR-10 Batch 3:  Batch Loss: 0.697501 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41584 Validation Accuracy: 0.497\n",
      "Epoch 84, CIFAR-10 Batch 4:  Batch Loss: 1.03045 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38753 Validation Accuracy: 0.4952\n",
      "Epoch 84, CIFAR-10 Batch 5:  Batch Loss: 0.934986 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39058 Validation Accuracy: 0.4976\n",
      "Epoch 85, CIFAR-10 Batch 1:  Batch Loss: 1.26602 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38403 Validation Accuracy: 0.5006\n",
      "Epoch 85, CIFAR-10 Batch 2:  Batch Loss: 1.00938 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.38291 Validation Accuracy: 0.497\n",
      "Epoch 85, CIFAR-10 Batch 3:  Batch Loss: 0.696848 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41374 Validation Accuracy: 0.4964\n",
      "Epoch 85, CIFAR-10 Batch 4:  Batch Loss: 1.02468 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38752 Validation Accuracy: 0.4954\n",
      "Epoch 85, CIFAR-10 Batch 5:  Batch Loss: 0.931838 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39046 Validation Accuracy: 0.4976\n",
      "Epoch 86, CIFAR-10 Batch 1:  Batch Loss: 1.26086 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38428 Validation Accuracy: 0.4966\n",
      "Epoch 86, CIFAR-10 Batch 2:  Batch Loss: 1.00745 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38229 Validation Accuracy: 0.4998\n",
      "Epoch 86, CIFAR-10 Batch 3:  Batch Loss: 0.69492 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41279 Validation Accuracy: 0.4984\n",
      "Epoch 86, CIFAR-10 Batch 4:  Batch Loss: 1.0259 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38966 Validation Accuracy: 0.4932\n",
      "Epoch 86, CIFAR-10 Batch 5:  Batch Loss: 0.932138 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39245 Validation Accuracy: 0.497\n",
      "Epoch 87, CIFAR-10 Batch 1:  Batch Loss: 1.25899 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38349 Validation Accuracy: 0.4994\n",
      "Epoch 87, CIFAR-10 Batch 2:  Batch Loss: 1.00831 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.37954 Validation Accuracy: 0.502\n",
      "Epoch 87, CIFAR-10 Batch 3:  Batch Loss: 0.691133 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41106 Validation Accuracy: 0.4986\n",
      "Epoch 87, CIFAR-10 Batch 4:  Batch Loss: 1.01793 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38761 Validation Accuracy: 0.4948\n",
      "Epoch 87, CIFAR-10 Batch 5:  Batch Loss: 0.924361 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.38877 Validation Accuracy: 0.499\n",
      "Epoch 88, CIFAR-10 Batch 1:  Batch Loss: 1.25286 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.3795 Validation Accuracy: 0.5008\n",
      "Epoch 88, CIFAR-10 Batch 2:  Batch Loss: 1.00458 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.3755 Validation Accuracy: 0.4996\n",
      "Epoch 88, CIFAR-10 Batch 3:  Batch Loss: 0.681516 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.40996 Validation Accuracy: 0.4998\n",
      "Epoch 88, CIFAR-10 Batch 4:  Batch Loss: 1.01789 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38661 Validation Accuracy: 0.4962\n",
      "Epoch 88, CIFAR-10 Batch 5:  Batch Loss: 0.920869 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.39031 Validation Accuracy: 0.4986\n",
      "Epoch 89, CIFAR-10 Batch 1:  Batch Loss: 1.25109 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38079 Validation Accuracy: 0.501\n",
      "Epoch 89, CIFAR-10 Batch 2:  Batch Loss: 1.00606 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.37786 Validation Accuracy: 0.5008\n",
      "Epoch 89, CIFAR-10 Batch 3:  Batch Loss: 0.678998 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.41066 Validation Accuracy: 0.4994\n",
      "Epoch 89, CIFAR-10 Batch 4:  Batch Loss: 1.01344 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38435 Validation Accuracy: 0.4984\n",
      "Epoch 89, CIFAR-10 Batch 5:  Batch Loss: 0.915847 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.38948 Validation Accuracy: 0.4994\n",
      "Epoch 90, CIFAR-10 Batch 1:  Batch Loss: 1.24654 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.37969 Validation Accuracy: 0.5022\n",
      "Epoch 90, CIFAR-10 Batch 2:  Batch Loss: 1.00284 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37555 Validation Accuracy: 0.5016\n",
      "Epoch 90, CIFAR-10 Batch 3:  Batch Loss: 0.677595 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.40737 Validation Accuracy: 0.5012\n",
      "Epoch 90, CIFAR-10 Batch 4:  Batch Loss: 1.01309 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.38529 Validation Accuracy: 0.4986\n",
      "Epoch 90, CIFAR-10 Batch 5:  Batch Loss: 0.90799 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.38139 Validation Accuracy: 0.5008\n",
      "Epoch 91, CIFAR-10 Batch 1:  Batch Loss: 1.23568 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38336 Validation Accuracy: 0.4984\n",
      "Epoch 91, CIFAR-10 Batch 2:  Batch Loss: 1.00901 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37569 Validation Accuracy: 0.5038\n",
      "Epoch 91, CIFAR-10 Batch 3:  Batch Loss: 0.66677 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.40786 Validation Accuracy: 0.5022\n",
      "Epoch 91, CIFAR-10 Batch 4:  Batch Loss: 1.01403 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.3881 Validation Accuracy: 0.5006\n",
      "Epoch 91, CIFAR-10 Batch 5:  Batch Loss: 0.900961 Batch Accuracy: 0.7\n",
      "Validation Loss: 1.38737 Validation Accuracy: 0.5004\n",
      "Epoch 92, CIFAR-10 Batch 1:  Batch Loss: 1.23859 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.38003 Validation Accuracy: 0.5024\n",
      "Epoch 92, CIFAR-10 Batch 2:  Batch Loss: 0.994325 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37456 Validation Accuracy: 0.5034\n",
      "Epoch 92, CIFAR-10 Batch 3:  Batch Loss: 0.662282 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40701 Validation Accuracy: 0.5042\n",
      "Epoch 92, CIFAR-10 Batch 4:  Batch Loss: 1.01054 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.3906 Validation Accuracy: 0.5012\n",
      "Epoch 92, CIFAR-10 Batch 5:  Batch Loss: 0.903111 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.38176 Validation Accuracy: 0.5004\n",
      "Epoch 93, CIFAR-10 Batch 1:  Batch Loss: 1.23914 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.37913 Validation Accuracy: 0.5028\n",
      "Epoch 93, CIFAR-10 Batch 2:  Batch Loss: 0.998185 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37476 Validation Accuracy: 0.5048\n",
      "Epoch 93, CIFAR-10 Batch 3:  Batch Loss: 0.662072 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40817 Validation Accuracy: 0.5034\n",
      "Epoch 93, CIFAR-10 Batch 4:  Batch Loss: 1.01138 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.38792 Validation Accuracy: 0.502\n",
      "Epoch 93, CIFAR-10 Batch 5:  Batch Loss: 0.894228 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.38224 Validation Accuracy: 0.502\n",
      "Epoch 94, CIFAR-10 Batch 1:  Batch Loss: 1.2331 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.38136 Validation Accuracy: 0.5018\n",
      "Epoch 94, CIFAR-10 Batch 2:  Batch Loss: 0.99851 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37288 Validation Accuracy: 0.5064\n",
      "Epoch 94, CIFAR-10 Batch 3:  Batch Loss: 0.656397 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40579 Validation Accuracy: 0.5056\n",
      "Epoch 94, CIFAR-10 Batch 4:  Batch Loss: 1.00772 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38815 Validation Accuracy: 0.4998\n",
      "Epoch 94, CIFAR-10 Batch 5:  Batch Loss: 0.886734 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.38216 Validation Accuracy: 0.5004\n",
      "Epoch 95, CIFAR-10 Batch 1:  Batch Loss: 1.22321 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.37771 Validation Accuracy: 0.5004\n",
      "Epoch 95, CIFAR-10 Batch 2:  Batch Loss: 0.99631 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37368 Validation Accuracy: 0.5068\n",
      "Epoch 95, CIFAR-10 Batch 3:  Batch Loss: 0.64862 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40577 Validation Accuracy: 0.5022\n",
      "Epoch 95, CIFAR-10 Batch 4:  Batch Loss: 1.00626 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.38976 Validation Accuracy: 0.5012\n",
      "Epoch 95, CIFAR-10 Batch 5:  Batch Loss: 0.880158 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.37874 Validation Accuracy: 0.5\n",
      "Epoch 96, CIFAR-10 Batch 1:  Batch Loss: 1.21284 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.37918 Validation Accuracy: 0.501\n",
      "Epoch 96, CIFAR-10 Batch 2:  Batch Loss: 0.995277 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.37022 Validation Accuracy: 0.5086\n",
      "Epoch 96, CIFAR-10 Batch 3:  Batch Loss: 0.6488 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40355 Validation Accuracy: 0.5024\n",
      "Epoch 96, CIFAR-10 Batch 4:  Batch Loss: 1.00203 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.38639 Validation Accuracy: 0.503\n",
      "Epoch 96, CIFAR-10 Batch 5:  Batch Loss: 0.878569 Batch Accuracy: 0.725\n",
      "Validation Loss: 1.37983 Validation Accuracy: 0.5026\n",
      "Epoch 97, CIFAR-10 Batch 1:  Batch Loss: 1.20734 Batch Accuracy: 0.45\n",
      "Validation Loss: 1.37756 Validation Accuracy: 0.503\n",
      "Epoch 97, CIFAR-10 Batch 2:  Batch Loss: 0.992313 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37137 Validation Accuracy: 0.506\n",
      "Epoch 97, CIFAR-10 Batch 3:  Batch Loss: 0.642171 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40351 Validation Accuracy: 0.5018\n",
      "Epoch 97, CIFAR-10 Batch 4:  Batch Loss: 1.00055 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.38861 Validation Accuracy: 0.503\n",
      "Epoch 97, CIFAR-10 Batch 5:  Batch Loss: 0.873577 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.37867 Validation Accuracy: 0.504\n",
      "Epoch 98, CIFAR-10 Batch 1:  Batch Loss: 1.20615 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.37715 Validation Accuracy: 0.5006\n",
      "Epoch 98, CIFAR-10 Batch 2:  Batch Loss: 0.989709 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.37033 Validation Accuracy: 0.5068\n",
      "Epoch 98, CIFAR-10 Batch 3:  Batch Loss: 0.642838 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40129 Validation Accuracy: 0.5018\n",
      "Epoch 98, CIFAR-10 Batch 4:  Batch Loss: 0.995422 Batch Accuracy: 0.6\n",
      "Validation Loss: 1.38787 Validation Accuracy: 0.5034\n",
      "Epoch 98, CIFAR-10 Batch 5:  Batch Loss: 0.871805 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.37831 Validation Accuracy: 0.504\n",
      "Epoch 99, CIFAR-10 Batch 1:  Batch Loss: 1.19776 Batch Accuracy: 0.475\n",
      "Validation Loss: 1.37667 Validation Accuracy: 0.5016\n",
      "Epoch 99, CIFAR-10 Batch 2:  Batch Loss: 0.988339 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.36963 Validation Accuracy: 0.5096\n",
      "Epoch 99, CIFAR-10 Batch 3:  Batch Loss: 0.639426 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.40516 Validation Accuracy: 0.502\n",
      "Epoch 99, CIFAR-10 Batch 4:  Batch Loss: 0.985503 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.3871 Validation Accuracy: 0.5036\n",
      "Epoch 99, CIFAR-10 Batch 5:  Batch Loss: 0.864888 Batch Accuracy: 0.775\n",
      "Validation Loss: 1.37711 Validation Accuracy: 0.5042\n",
      "Epoch 100, CIFAR-10 Batch 1:  Batch Loss: 1.1954 Batch Accuracy: 0.5\n",
      "Validation Loss: 1.37981 Validation Accuracy: 0.5028\n",
      "Epoch 100, CIFAR-10 Batch 2:  Batch Loss: 0.98485 Batch Accuracy: 0.65\n",
      "Validation Loss: 1.36968 Validation Accuracy: 0.5056\n",
      "Epoch 100, CIFAR-10 Batch 3:  Batch Loss: 0.632367 Batch Accuracy: 0.75\n",
      "Validation Loss: 1.4026 Validation Accuracy: 0.503\n",
      "Epoch 100, CIFAR-10 Batch 4:  Batch Loss: 0.989486 Batch Accuracy: 0.625\n",
      "Validation Loss: 1.38589 Validation Accuracy: 0.504\n",
      "Epoch 100, CIFAR-10 Batch 5:  Batch Loss: 0.865148 Batch Accuracy: 0.7\n",
      "Validation Loss: 1.37943 Validation Accuracy: 0.5046\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5088014240506329\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQk0uRYRERERCTS5FhEREREJPr/JCpYCBEyyAAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6d1794d30>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
