# My Data Science Projects
Adventures in Artificial Intelligence, Machine Learning, Deep Learning, AI for Finance

## AI for Trading
I completed six month AI for Trading Nanodegree from Udacity in April 2019. The following eight projects were completed as part of it.

## Backtesting
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/8-backtesting/project_8_starter.ipynb), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/8-backtesting/project_8_starter.ipynb)

In this project, I built a fairly realistic backtester that uses the Barra data. The backtester performed portfolio optimization that included transaction costs. I also implemented performance attribution to identify the major drivers of the portfolio's profit-and-loss (PnL). This project was done as part of my Udacity AI for Trading Nanodegree.

## Combining Signals for Advanced Alpha
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/7-combining-signals-for-enhanced-alpha), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/7-combining-signals-for-enhanced-alpha/project_7_starter.ipynb)

In this project, I combined signals on a random forest for enhanced alpha. I also implemented logic to solve the problem of overlapping samples. For the dataset, I used the end of day from Quotemedia and sector data from Sharadar. This project was done as part of my Udacity AI for Trading Nanodegree.

## Sentiment Analysis with Neural Networks
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/6-sentiment-analysis-with-neural-networks), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/6-sentiment-analysis-with-neural-networks/project_6_starter.ipynb)

In this project, I built a deep learning model to classify the sentiment of messages from StockTwits, a social network for investors and traders. The model predicted if any particular message is positive or negative. From this, I was able to generate a signal of the public sentiment for various ticker symbols. This project was done as part of my Udacity AI for Trading Nanodegree.

### Natural Language Processing (NLP) on Financial Statements
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/5-nlp-on-financial-statements), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/5-nlp-on-financial-statements/project_5_starter.ipynb)

In this project, I impmemented NLP Analysis on 10-k financial statements to generate an alpha factor. For the dataset, I used the end of day from Quotemedia and Loughran-McDonald sentiment word lists. This project was done as part of my Udacity AI for Trading Nanodegree.

### Alpha Research and Multi-Factor Modeling
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/4-alpha-research-and-factor-modeling), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/4-alpha-research-and-factor-modeling/project_4_starter.ipynb)

In this project, I built a statistical risk model using PCA and used it to build a portfolio along with 5 alpha factors. I evaluated the factors using factor-weighted returns, quantile analysis, sharpe ratio, and turnover analysis. Finally I optimized the portfolio using the risk model and factors using multiple optimization formulations. For the dataset, I used the end of day from Quotemedia and sector data from Sharadar. This project was done as part of my Udacity AI for Trading Nanodegree.

### Smart Beta Portfolio and Portfolio Optimization
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/3-smart-beta-portfolio-and-portfolio-optimization), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/3-smart-beta-portfolio-and-portfolio-optimization/project_3_starter.ipynb)

In this project, I built a smart beta portfolio and compared it to a benchmark index. To find out how well the smart beta portfolio did, I calculated the tracking error against the index. Subsequently I  built a portfolio by using quadratic programming to optimize the weights. I rebalanced this portfolio and calculated turn over to evaluate its performance. For the dataset, I used the end of day from Quotemedia. This project was done as part of my Udacity AI for Trading Nanodegree.

### Breakout Strategy
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/2-breakout-strategy), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/2-breakout-strategy/project_2_starter.ipynb)

In this project, I implemented the [breakout strategy](https://www.investopedia.com/articles/trading/08/trading-breakouts.asp), found and removed outliers and subsequently tested it to see if it has the potential to be profitable using a Histogram and P-Value. For the dataset, I used the end of day from Quotemedia. This project was done as part of my Udacity AI for Trading Nanodegree.

### Trading with Momentum
[github](https://github.com/ark4innovation/datascience/tree/master/ai-for-trading/1-trading-with-momentum), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/ai-for-trading/1-trading-with-momentum/project_1_starter.ipynb)

In this project, I implemented a trading signal based on a momentum indicator and computed the signal for the time range given and applied it to the dataset to produce projected returns. Subsequently I performed statistical tests on the mean of the returns to conclude if there is alpha in the signal. For the dataset, I used the end of day from Quotemedia. This project was done as part of my Udacity AI for Trading Nanodegree.


## Deep Learning
I completed six month Deep Learning Foundations Nanodegree in August 2017. The following projects were completed as part of it.

### Face Generation
[github](https://github.com/ark4innovation/datascience/tree/master/deep-learning/udacity-projects/face-generation), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/deep-learning/udacity-projects/face-generation/dlnd_face_generation.ipynb)

This project was done as part of my Udacity Deep-Learning Nanodegree. In this project, I built a generative adversarial network and trained it on CelebA dataset in order to generate new images of faces.

### Language Translation
[github](https://github.com/ark4innovation/datascience/tree/master/deep-learning/udacity-projects/language-translation), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/deep-learning/udacity-projects/language-translation/dlnd_language_translation.ipynb)

This project was done as part of my Udacity Deep-Learning Nanodegree. In this project, I built trained a sequence to sequence model on a dataset of English and French sentences in order to translate new sentences from English to French.

### TV Script Generation
[github](https://github.com/ark4innovation/datascience/blob/master/deep-learning/udacity-projects/tv-script-generation), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/deep-learning/udacity-projects/tv-script-generation/dlnd_tv_script_generation.ipynb)

This project was done as part of my Udacity Deep-Learning Nanodegree. In this project, I generated my own Simpsons TV scripts using RNNs. I used part of the Simpsons dataset of scripts from 27 seasons. The Neural Network generated a new TV script for a scene at Moe's Tavern.

### Image Classification
[github](https://github.com/ark4innovation/datascience/blob/master/deep-learning/udacity-projects/image-classification), [nbviewer](http://nbviewer.jupyter.org/github/ark4innovation/datascience/blob/master/deep-learning/udacity-projects/image-classification/dlnd_image_classification.ipynb)

This project was done as part of my Udacity Deep-Learning Nanodegree. In this project, I classified images from the CIFAR-10 dataset. The dataset consisted of airplanes, dogs, cats, and other objects. I preprocessed the images, then trained a convolutional neural network on all the samples. The images were normalized and the labels one-hot encoded. Then I built a neural network with convolutional, max pooling, dropout, and fully connected layers to predict on sample images.
